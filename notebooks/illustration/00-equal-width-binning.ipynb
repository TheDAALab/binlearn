{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bd83ca",
   "metadata": {},
   "source": [
    "# EqualWidthBinning Illustration\n",
    "\n",
    "This notebook demonstrates the complete workflow of using EqualWidthBinning from the binning package, including:\n",
    "\n",
    "1. Basic usage with different input formats (numpy arrays, pandas DataFrames, polars DataFrames)\n",
    "2. Various output formats and transformations\n",
    "3. sklearn integration and pipeline usage\n",
    "4. Serialization and parameter transfer\n",
    "5. Advanced features and edge cases\n",
    "\n",
    "Let's start by importing the necessary libraries and creating sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babd0aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Polars available: True\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    import polars as pl\n",
    "    POLARS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    POLARS_AVAILABLE = False\n",
    "    print(\"Polars not available - skipping polars examples\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Import binning classes\n",
    "from binning.methods import EqualWidthBinning\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Polars available: {POLARS_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ed6f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created:\n",
      "NumPy array shape: (1000, 4)\n",
      "Pandas DataFrame shape: (1000, 4)\n",
      "Polars DataFrame shape: (1000, 4)\n",
      "Target distribution: [481 519]\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data with different distributions\n",
    "n_samples = 1000\n",
    "\n",
    "# Feature 1: Normal distribution\n",
    "feature1 = np.random.normal(50, 15, n_samples)\n",
    "\n",
    "# Feature 2: Exponential distribution\n",
    "feature2 = np.random.exponential(2, n_samples)\n",
    "\n",
    "# Feature 3: Uniform distribution\n",
    "feature3 = np.random.uniform(0, 100, n_samples)\n",
    "\n",
    "# Feature 4: Bimodal distribution\n",
    "feature4 = np.concatenate([\n",
    "    np.random.normal(20, 5, n_samples//2),\n",
    "    np.random.normal(80, 5, n_samples//2)\n",
    "])\n",
    "\n",
    "# Create target variable for classification\n",
    "target = (feature1 + feature2 + feature3 + feature4 > 150).astype(int)\n",
    "\n",
    "# Create datasets in different formats\n",
    "# 1. NumPy array\n",
    "X_numpy = np.column_stack([feature1, feature2, feature3, feature4])\n",
    "\n",
    "# 2. Pandas DataFrame\n",
    "X_pandas = pd.DataFrame({\n",
    "    'normal_feature': feature1,\n",
    "    'exponential_feature': feature2,\n",
    "    'uniform_feature': feature3,\n",
    "    'bimodal_feature': feature4\n",
    "})\n",
    "\n",
    "# 3. Polars DataFrame (if available)\n",
    "if POLARS_AVAILABLE:\n",
    "    X_polars = pl.DataFrame({\n",
    "        'normal_feature': feature1,\n",
    "        'exponential_feature': feature2,\n",
    "        'uniform_feature': feature3,\n",
    "        'bimodal_feature': feature4\n",
    "    })\n",
    "\n",
    "print(\"Sample data created:\")\n",
    "print(f\"NumPy array shape: {X_numpy.shape}\")\n",
    "print(f\"Pandas DataFrame shape: {X_pandas.shape}\")\n",
    "if POLARS_AVAILABLE:\n",
    "    print(f\"Polars DataFrame shape: {X_polars.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd35c1",
   "metadata": {},
   "source": [
    "## 1. Basic Usage with Different Input Formats\n",
    "\n",
    "Let's start with the basic workflow of EqualWidthBinning, showing how it works with different input formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b4ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NumPy Array Example ===\n",
      "Original data shape: (1000, 4)\n",
      "Binned data shape: (1000, 4)\n",
      "Original data sample:\n",
      "[[57.4507123   0.36660227 21.90688091 16.96649853]\n",
      " [47.92603548  0.22089763  3.67213625 21.05641846]\n",
      " [59.71532807  2.02356823 10.80257541 26.00039478]]\n",
      "Binned data sample:\n",
      "[[2 0 1 0]\n",
      " [2 0 0 0]\n",
      " [2 0 0 1]]\n",
      "\n",
      "Bin edges for each column:\n",
      "Column 0: [np.float64(1.380989898963911), np.float64(22.66298639113529), np.float64(43.94498288330667), np.float64(65.22697937547805), np.float64(86.50897586764944), np.float64(107.79097235982081)]\n",
      "Column 1: [np.float64(0.00644690670446925), np.float64(2.9818466892453372), np.float64(5.957246471786205), np.float64(8.932646254327073), np.float64(11.908046036867942), np.float64(14.883445819408811)]\n",
      "Column 2: [np.float64(0.0011634755366141114), np.float64(19.957347894068853), np.float64(39.91353231260109), np.float64(59.86971673113333), np.float64(79.82590114966557), np.float64(99.78208556819781)]\n",
      "Column 3: [np.float64(5.044320145496384), np.float64(22.903859172063875), np.float64(40.76339819863137), np.float64(58.62293722519887), np.float64(76.48247625176636), np.float64(94.34201527833385)]\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Basic usage with NumPy arrays\n",
    "print(\"=== NumPy Array Example ===\")\n",
    "\n",
    "# Initialize the binning transformer\n",
    "binning_numpy = EqualWidthBinning(n_bins=5)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_binned_numpy = binning_numpy.fit_transform(X_numpy)\n",
    "\n",
    "print(f\"Original data shape: {X_numpy.shape}\")\n",
    "print(f\"Binned data shape: {X_binned_numpy.shape}\")\n",
    "print(f\"Original data sample:\\n{X_numpy[:3]}\")\n",
    "print(f\"Binned data sample:\\n{X_binned_numpy[:3]}\")\n",
    "\n",
    "# Check the bin edges\n",
    "print(f\"\\nBin edges for each column:\")\n",
    "for i, edges in enumerate(binning_numpy._bin_edges.values()):\n",
    "    print(f\"Column {i}: {edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7568038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pandas DataFrame Example ===\n",
      "Original DataFrame:\n",
      "   normal_feature  exponential_feature  uniform_feature  bimodal_feature\n",
      "0       57.450712             0.366602        21.906881        16.966499\n",
      "1       47.926035             0.220898         3.672136        21.056418\n",
      "2       59.715328             2.023568        10.802575        26.000395\n",
      "3       72.845448             2.451590        33.886065        17.540488\n",
      "4       46.487699             0.064191        80.258568        10.617236\n",
      "\n",
      "Binned DataFrame:\n",
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [2 0 1 0]\n",
      " [1 0 2 0]]\n",
      "\n",
      "Data types - Original: [dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64')]\n",
      "Data types - Binned: int64\n",
      "\n",
      "Output type: numpy array (not DataFrame)\n",
      "\n",
      "Original data statistics:\n",
      "       normal_feature  exponential_feature  uniform_feature  bimodal_feature\n",
      "count     1000.000000          1000.000000      1000.000000      1000.000000\n",
      "mean        50.289981             2.015972        49.449499        50.036445\n",
      "std         14.688239             2.005977        28.891967        30.536434\n",
      "min          1.380990             0.006447         0.001163         5.044320\n",
      "25%         40.286145             0.567909        25.652351        19.844230\n",
      "50%         50.379509             1.451862        49.171060        52.491973\n",
      "75%         59.719158             2.743369        73.862888        80.211500\n",
      "max        107.790972            14.883446        99.782086        94.342015\n",
      "\n",
      "Binned data statistics:\n",
      "       normal_feature  exponential_feature  uniform_feature  bimodal_feature\n",
      "count     1000.000000          1000.000000      1000.000000      1000.000000\n",
      "mean         0.899000             0.086000         1.001000         1.001000\n",
      "std          0.492993             0.307732         0.818333         0.999999\n",
      "min          0.000000             0.000000         0.000000         0.000000\n",
      "25%          1.000000             0.000000         0.000000         0.000000\n",
      "50%          1.000000             0.000000         1.000000         1.500000\n",
      "75%          1.000000             0.000000         2.000000         2.000000\n",
      "max          2.000000             2.000000         2.000000         2.000000\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Usage with Pandas DataFrames\n",
    "print(\"=== Pandas DataFrame Example ===\")\n",
    "\n",
    "# Initialize with different parameters\n",
    "binning_pandas = EqualWidthBinning(n_bins=3)\n",
    "\n",
    "# Fit and transform\n",
    "X_binned_pandas = binning_pandas.fit_transform(X_pandas)\n",
    "\n",
    "print(f\"Original DataFrame:\\n{X_pandas.head()}\")\n",
    "print(f\"\\nBinned DataFrame:\")\n",
    "if isinstance(X_binned_pandas, pd.DataFrame):\n",
    "    print(X_binned_pandas.head())\n",
    "    print(f\"\\nData types - Original: {X_pandas.dtypes.tolist()}\")\n",
    "    print(f\"Data types - Binned: {X_binned_pandas.dtypes.tolist()}\")\n",
    "    # Show column names are preserved\n",
    "    print(f\"\\nColumn names preserved: {list(X_pandas.columns) == list(X_binned_pandas.columns)}\")\n",
    "else:\n",
    "    # It's a numpy array\n",
    "    print(X_binned_pandas[:5])  # Show first 5 rows\n",
    "    print(f\"\\nData types - Original: {X_pandas.dtypes.tolist()}\")\n",
    "    print(f\"Data types - Binned: {X_binned_pandas.dtype}\")\n",
    "    print(f\"\\nOutput type: numpy array (not DataFrame)\")\n",
    "\n",
    "# Check statistics\n",
    "print(f\"\\nOriginal data statistics:\")\n",
    "print(X_pandas.describe())\n",
    "print(f\"\\nBinned data statistics:\")\n",
    "if isinstance(X_binned_pandas, pd.DataFrame):\n",
    "    print(X_binned_pandas.describe())\n",
    "else:\n",
    "    # Create a temporary DataFrame for statistics display\n",
    "    temp_df = pd.DataFrame(X_binned_pandas, columns=X_pandas.columns)\n",
    "    print(temp_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883d3c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Polars DataFrame Example ===\n",
      "Original Polars DataFrame:\n",
      "shape: (5, 4)\n",
      "┌────────────────┬─────────────────────┬─────────────────┬─────────────────┐\n",
      "│ normal_feature ┆ exponential_feature ┆ uniform_feature ┆ bimodal_feature │\n",
      "│ ---            ┆ ---                 ┆ ---             ┆ ---             │\n",
      "│ f64            ┆ f64                 ┆ f64             ┆ f64             │\n",
      "╞════════════════╪═════════════════════╪═════════════════╪═════════════════╡\n",
      "│ 57.450712      ┆ 0.366602            ┆ 21.906881       ┆ 16.966499       │\n",
      "│ 47.926035      ┆ 0.220898            ┆ 3.672136        ┆ 21.056418       │\n",
      "│ 59.715328      ┆ 2.023568            ┆ 10.802575       ┆ 26.000395       │\n",
      "│ 72.845448      ┆ 2.45159             ┆ 33.886065       ┆ 17.540488       │\n",
      "│ 46.487699      ┆ 0.064191            ┆ 80.258568       ┆ 10.617236       │\n",
      "└────────────────┴─────────────────────┴─────────────────┴─────────────────┘\n",
      "\n",
      "Binned Polars DataFrame:\n",
      "Output type: numpy array\n",
      "[[2 0 0 0]\n",
      " [1 0 0 0]\n",
      " [2 0 0 0]\n",
      " [2 0 1 0]\n",
      " [1 0 3 0]]\n",
      "\n",
      "Data types - Original: [Float64, Float64, Float64, Float64]\n",
      "Data types - Binned: int64\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Usage with Polars DataFrames (if available)\n",
    "if POLARS_AVAILABLE:\n",
    "    print(\"=== Polars DataFrame Example ===\")\n",
    "    \n",
    "    binning_polars = EqualWidthBinning(n_bins=4)\n",
    "    X_binned_polars = binning_polars.fit_transform(X_polars)\n",
    "    \n",
    "    print(f\"Original Polars DataFrame:\")\n",
    "    print(X_polars.head())\n",
    "    print(f\"\\nBinned Polars DataFrame:\")\n",
    "    if hasattr(X_binned_polars, 'head'):\n",
    "        # It's still a polars DataFrame\n",
    "        print(X_binned_polars.head())\n",
    "        print(f\"\\nData types - Original: {X_polars.dtypes}\")\n",
    "        print(f\"Data types - Binned: {X_binned_polars.dtypes}\")\n",
    "    else:\n",
    "        # It's a numpy array\n",
    "        print(\"Output type: numpy array\")\n",
    "        print(X_binned_polars[:5])  # Show first 5 rows\n",
    "        print(f\"\\nData types - Original: {X_polars.dtypes}\")\n",
    "        print(f\"Data types - Binned: {X_binned_polars.dtype}\")\n",
    "else:\n",
    "    print(\"Polars not available - skipping polars example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283ab52",
   "metadata": {},
   "source": [
    "## 2. Different Output Formats and Transformations\n",
    "\n",
    "EqualWidthBinning supports various output formats and transformation modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903c783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Different Numbers of Bins ===\n",
      "\n",
      "With 2 bins:\n",
      "Unique values: [0 1]\n",
      "Bin edges: [np.float64(1.380989898963911), np.float64(54.58598112939236), np.float64(107.79097235982081)]\n",
      "\n",
      "With 5 bins:\n",
      "Unique values: [0 1 2 3 4]\n",
      "Bin edges: [np.float64(1.380989898963911), np.float64(22.66298639113529), np.float64(43.94498288330667), np.float64(65.22697937547805), np.float64(86.50897586764944), np.float64(107.79097235982081)]\n",
      "\n",
      "With 10 bins:\n",
      "Unique values: [0 1 2 3 4 5 6 7 8 9]\n",
      "Bin edges: [np.float64(1.380989898963911), np.float64(12.021988145049601), np.float64(22.66298639113529), np.float64(33.30398463722098), np.float64(43.94498288330667), np.float64(54.58598112939236), np.float64(65.22697937547805), np.float64(75.86797762156374), np.float64(86.50897586764944), np.float64(97.14997411373511), np.float64(107.79097235982081)]\n",
      "\n",
      "=== Edge Cases ===\n",
      "Constant data binned: unique values = [2]\n",
      "\n",
      "=== Separate Fit and Transform ===\n",
      "Training data binned shape: (700, 4)\n",
      "Test data binned shape: (300, 4)\n",
      "Bin edges determined from training data:\n",
      "  Column normal_feature: [1.38, 96.18] with 3 bins\n",
      "  Column exponential_feature: [0.01, 12.81] with 3 bins\n",
      "  Column uniform_feature: [0.00, 99.78] with 3 bins\n",
      "  Column bimodal_feature: [5.84, 94.34] with 3 bins\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Different numbers of bins\n",
    "print(\"=== Different Numbers of Bins ===\")\n",
    "\n",
    "# Try different n_bins values\n",
    "for n_bins in [2, 5, 10]:\n",
    "    binning = EqualWidthBinning(n_bins=n_bins)\n",
    "    X_binned = binning.fit_transform(X_pandas.iloc[:, 0:1])  # Just first column\n",
    "    \n",
    "    print(f\"\\nWith {n_bins} bins:\")\n",
    "    # Convert to pandas if it's numpy for consistent access\n",
    "    if isinstance(X_binned, np.ndarray):\n",
    "        unique_values = np.unique(X_binned[:, 0])\n",
    "    else:\n",
    "        unique_values = np.unique(X_binned.iloc[:, 0])\n",
    "    print(f\"Unique values: {unique_values}\")\n",
    "    print(f\"Bin edges: {list(binning._bin_edges.values())[0]}\")\n",
    "\n",
    "# 2.2 Handling edge cases and warnings\n",
    "print(\"\\n=== Edge Cases ===\")\n",
    "\n",
    "# Data with constant values (should trigger warning)\n",
    "constant_data = pd.DataFrame({'constant': np.ones(100)})\n",
    "binning_constant = EqualWidthBinning(n_bins=5)\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    X_constant_binned = binning_constant.fit_transform(constant_data)\n",
    "    \n",
    "    if w:\n",
    "        print(f\"Warning caught: {w[0].message}\")\n",
    "    # Handle both numpy and pandas outputs\n",
    "    if isinstance(X_constant_binned, np.ndarray):\n",
    "        unique_values = np.unique(X_constant_binned[:, 0])\n",
    "    else:\n",
    "        unique_values = np.unique(X_constant_binned.iloc[:, 0])\n",
    "    print(f\"Constant data binned: unique values = {unique_values}\")\n",
    "\n",
    "# 2.3 Separate fit and transform\n",
    "print(\"\\n=== Separate Fit and Transform ===\")\n",
    "\n",
    "binning_separate = EqualWidthBinning(n_bins=3)\n",
    "\n",
    "# Fit on training data\n",
    "X_train, X_test = train_test_split(X_pandas, test_size=0.3, random_state=42)\n",
    "binning_separate.fit(X_train)\n",
    "\n",
    "# Transform both training and test data using the same bins\n",
    "X_train_binned = binning_separate.transform(X_train)\n",
    "X_test_binned = binning_separate.transform(X_test)\n",
    "\n",
    "print(f\"Training data binned shape: {X_train_binned.shape}\")\n",
    "print(f\"Test data binned shape: {X_test_binned.shape}\")\n",
    "print(f\"Bin edges determined from training data:\")\n",
    "for i, (col_id, edges) in enumerate(binning_separate._bin_edges.items()):\n",
    "    print(f\"  Column {col_id}: [{edges[0]:.2f}, {edges[-1]:.2f}] with {len(edges)-1} bins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4434b",
   "metadata": {},
   "source": [
    "## 3. Scikit-learn Integration\n",
    "\n",
    "EqualWidthBinning is fully compatible with scikit-learn pipelines and workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ca7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scikit-learn Pipeline Integration ===\n",
      "Pipeline accuracy: 0.9300\n",
      "Binning fitted: True\n",
      "Number of columns processed: 4\n",
      "\n",
      "=== Manual sklearn-style usage ===\n",
      "Initially fitted: False\n",
      "After fit: True\n",
      "Transformed training data shape: (700, 4)\n",
      "Transformed test data shape: (300, 4)\n",
      "Training data bin ranges:\n",
      "  normal_feature: [0, 3]\n",
      "  exponential_feature: [0, 3]\n",
      "  uniform_feature: [0, 3]\n",
      "  bimodal_feature: [0, 3]\n",
      "Test data bin ranges:\n",
      "  normal_feature: [0, 3]\n",
      "  exponential_feature: [0, 3]\n",
      "  uniform_feature: [0, 3]\n",
      "  bimodal_feature: [0, 3]\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Pipeline Integration\n",
    "print(\"=== Scikit-learn Pipeline Integration ===\")\n",
    "\n",
    "# Create a pipeline with binning and classification\n",
    "pipeline = Pipeline([\n",
    "    ('binning', EqualWidthBinning(n_bins=5)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "])\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train_pipe, X_test_pipe, y_train, y_test = train_test_split(\n",
    "    X_pandas, target, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train_pipe, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test_pipe)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Pipeline accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Access the fitted binning transformer\n",
    "fitted_binning = pipeline.named_steps['binning']\n",
    "print(f\"Binning fitted: {fitted_binning._fitted}\")\n",
    "print(f\"Number of columns processed: {len(fitted_binning._bin_edges)}\")\n",
    "\n",
    "# 3.2 Manual sklearn-style usage\n",
    "print(\"\\n=== Manual sklearn-style usage ===\")\n",
    "\n",
    "# Initialize binning\n",
    "binning_sklearn = EqualWidthBinning(n_bins=4)\n",
    "\n",
    "# Check if fitted (should be False initially)\n",
    "print(f\"Initially fitted: {hasattr(binning_sklearn, '_fitted') and binning_sklearn._fitted}\")\n",
    "\n",
    "# Fit the binning\n",
    "binning_sklearn.fit(X_train_pipe)\n",
    "\n",
    "# Check if fitted (should be True after fitting)\n",
    "print(f\"After fit: {binning_sklearn._fitted}\")\n",
    "\n",
    "# Transform data\n",
    "X_train_transformed = binning_sklearn.transform(X_train_pipe)\n",
    "X_test_transformed = binning_sklearn.transform(X_test_pipe)\n",
    "\n",
    "print(f\"Transformed training data shape: {X_train_transformed.shape}\")\n",
    "print(f\"Transformed test data shape: {X_test_transformed.shape}\")\n",
    "\n",
    "# Verify consistent binning\n",
    "print(f\"Training data bin ranges:\")\n",
    "if isinstance(X_train_transformed, pd.DataFrame):\n",
    "    for col in X_train_transformed.columns:\n",
    "        print(f\"  {col}: [{X_train_transformed[col].min()}, {X_train_transformed[col].max()}]\")\n",
    "else:\n",
    "    for i in range(X_train_transformed.shape[1]):\n",
    "        col_name = X_pandas.columns[i] if i < len(X_pandas.columns) else f\"col_{i}\"\n",
    "        print(f\"  {col_name}: [{X_train_transformed[:, i].min()}, {X_train_transformed[:, i].max()}]\")\n",
    "\n",
    "print(f\"Test data bin ranges:\")\n",
    "if isinstance(X_test_transformed, pd.DataFrame):\n",
    "    for col in X_test_transformed.columns:\n",
    "        print(f\"  {col}: [{X_test_transformed[col].min()}, {X_test_transformed[col].max()}]\")\n",
    "else:\n",
    "    for i in range(X_test_transformed.shape[1]):\n",
    "        col_name = X_pandas.columns[i] if i < len(X_pandas.columns) else f\"col_{i}\"\n",
    "        print(f\"  {col_name}: [{X_test_transformed[:, i].min()}, {X_test_transformed[:, i].max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386fd1a",
   "metadata": {},
   "source": [
    "## 4. Serialization and Parameter Transfer\n",
    "\n",
    "One of the key features of EqualWidthBinning is the ability to serialize fitted models and recreate them without refitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b0f89fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parameter Transfer Workflow ===\n",
      "Original binning result:\n",
      "[[3 0 1 0]\n",
      " [3 0 0 1]\n",
      " [3 0 0 1]\n",
      " [4 1 2 0]\n",
      " [2 0 5 0]]\n",
      "\n",
      "Fitted parameters keys: ['bin_edges', 'bin_range', 'bin_representatives', 'clip', 'fit_jointly', 'n_bins', 'preserve_dataframe']\n",
      "\n",
      "New binning result (ready to use):\n",
      "[[3 0 1 0]\n",
      " [3 0 0 1]\n",
      " [3 0 0 1]\n",
      " [4 1 2 0]\n",
      " [2 0 5 0]]\n",
      "\n",
      "Results are identical: True\n",
      "New binning is fitted: True\n",
      "\n",
      "=== Detailed Parameter Inspection ===\n",
      "n_bins: 7\n",
      "bin_range: None\n",
      "bin_edges: Dict with 4 columns, e.g., column normal_feature has 8 values\n",
      "bin_representatives: Dict with 4 columns, e.g., column normal_feature has 7 values\n",
      "clip: True\n",
      "preserve_dataframe: False\n",
      "\n",
      "=== Robustness Test ===\n",
      "Test data:\n",
      "   normal_feature  exponential_feature  uniform_feature  bimodal_feature\n",
      "0              25                  0.5               10               15\n",
      "1              75                  2.0               90               85\n",
      "2              50                  1.0               50               50\n",
      "3             100                  5.0              100               95\n",
      "4               0                  0.1                0                5\n",
      "\n",
      "Original binning on test data:\n",
      "[[1 0 0 0]\n",
      " [4 0 6 6]\n",
      " [3 0 3 3]\n",
      " [6 2 6 6]\n",
      " [0 0 0 0]]\n",
      "\n",
      "New binning on test data:\n",
      "[[1 0 0 0]\n",
      " [4 0 6 6]\n",
      " [3 0 3 3]\n",
      " [6 2 6 6]\n",
      " [0 0 0 0]]\n",
      "\n",
      "Test results identical: True\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Parameter Transfer Workflow\n",
    "print(\"=== Parameter Transfer Workflow ===\")\n",
    "\n",
    "# Step 1: Create and fit original binning\n",
    "original_binning = EqualWidthBinning(n_bins=7)\n",
    "original_binning.fit(X_pandas)\n",
    "\n",
    "# Transform some data with the original\n",
    "original_result = original_binning.transform(X_pandas.iloc[:5])\n",
    "print(\"Original binning result:\")\n",
    "print(original_result)\n",
    "\n",
    "# Step 2: Get parameters from fitted binning\n",
    "fitted_params = original_binning.get_params()\n",
    "print(f\"\\nFitted parameters keys: {list(fitted_params.keys())}\")\n",
    "\n",
    "# Step 3: Create new binning instance with these parameters\n",
    "# When bin_edges are provided in constructor, the binning is automatically fitted\n",
    "new_binning = EqualWidthBinning(**fitted_params)\n",
    "\n",
    "# Step 4: Verify the new binning produces identical results WITHOUT additional fitting\n",
    "new_result = new_binning.transform(X_pandas.iloc[:5])\n",
    "print(f\"\\nNew binning result (ready to use):\")\n",
    "print(new_result)\n",
    "\n",
    "# Step 5: Verify results are identical\n",
    "results_identical = np.allclose(original_result, new_result)\n",
    "print(f\"\\nResults are identical: {results_identical}\")\n",
    "print(f\"New binning is fitted: {new_binning._fitted}\")\n",
    "\n",
    "# 4.2 Detailed Parameter Inspection\n",
    "print(\"\\n=== Detailed Parameter Inspection ===\")\n",
    "\n",
    "# Show the key parameters that enable reconstruction\n",
    "important_params = ['n_bins', 'bin_range', 'bin_edges', 'bin_representatives', 'clip', 'preserve_dataframe']\n",
    "for key in important_params:\n",
    "    if key in fitted_params:\n",
    "        value = fitted_params[key]\n",
    "        if isinstance(value, dict) and len(value) > 0:\n",
    "            # For dictionaries, show structure\n",
    "            first_key, first_val = next(iter(value.items()))\n",
    "            if isinstance(first_val, (list, np.ndarray)):\n",
    "                print(f\"{key}: Dict with {len(value)} columns, e.g., column {first_key} has {len(first_val)} values\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "# 4.3 Robustness Test\n",
    "print(\"\\n=== Robustness Test ===\")\n",
    "\n",
    "# Test with different data to ensure both binnings behave identically\n",
    "test_data = pd.DataFrame({\n",
    "    'normal_feature': [25, 75, 50, 100, 0],\n",
    "    'exponential_feature': [0.5, 2.0, 1.0, 5.0, 0.1],\n",
    "    'uniform_feature': [10, 90, 50, 100, 0],\n",
    "    'bimodal_feature': [15, 85, 50, 95, 5]\n",
    "})\n",
    "\n",
    "original_test_result = original_binning.transform(test_data)\n",
    "new_test_result = new_binning.transform(test_data)\n",
    "\n",
    "print(\"Test data:\")\n",
    "print(test_data)\n",
    "print(f\"\\nOriginal binning on test data:\")\n",
    "print(original_test_result)\n",
    "print(f\"\\nNew binning on test data:\")\n",
    "print(new_test_result)\n",
    "\n",
    "# Compare results\n",
    "test_results_identical = np.allclose(original_test_result, new_test_result)\n",
    "print(f\"\\nTest results identical: {test_results_identical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27ff1efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Practical Serialization Examples ===\n",
      "1. JSON Serialization:\n",
      "JSON configuration (first 200 chars): {\n",
      "  \"bin_edges\": {\n",
      "    \"normal_feature\": [\n",
      "      1.380989898963911,\n",
      "      16.58241596480061,\n",
      "      31.78384203063731,\n",
      "      46.98526809647401,\n",
      "      62.18669416231071,\n",
      "      77.38812022814741,\n",
      "      9...\n",
      "JSON-restored binning works: True\n",
      "\n",
      "2. Pickle Serialization (for Python objects):\n",
      "Pickled binning works: True\n",
      "Pickled binning is fitted: True\n",
      "\n",
      "3. Custom Serialization Function:\n",
      "Custom serialization works: True\n",
      "\n",
      "4. Summary:\n",
      "✓ Parameter transfer enables object reconstruction\n",
      "✓ JSON serialization works for configuration storage\n",
      "✓ Pickle serialization preserves complete object state\n",
      "✓ Custom serialization allows version control and validation\n",
      "✓ All methods produce identical transformation results\n"
     ]
    }
   ],
   "source": [
    "# 4.4 Practical Serialization Examples\n",
    "print(\"=== Practical Serialization Examples ===\")\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Example 1: JSON serialization (for configuration files)\n",
    "print(\"1. JSON Serialization:\")\n",
    "\n",
    "# Get parameters and convert numpy arrays to lists for JSON compatibility\n",
    "params_for_json = original_binning.get_params()\n",
    "json_compatible_params = {}\n",
    "\n",
    "for key, value in params_for_json.items():\n",
    "    if isinstance(value, dict):\n",
    "        # Handle dictionary parameters (like bin_edges)\n",
    "        json_compatible_params[key] = {\n",
    "            k: v.tolist() if hasattr(v, 'tolist') else v \n",
    "            for k, v in value.items()\n",
    "        }\n",
    "    elif hasattr(value, 'tolist'):\n",
    "        # Handle numpy arrays\n",
    "        json_compatible_params[key] = value.tolist()\n",
    "    else:\n",
    "        # Handle regular parameters\n",
    "        json_compatible_params[key] = value\n",
    "\n",
    "# Simulate saving to JSON file\n",
    "json_str = json.dumps(json_compatible_params, indent=2)\n",
    "print(f\"JSON configuration (first 200 chars): {json_str[:200]}...\")\n",
    "\n",
    "# Simulate loading from JSON\n",
    "loaded_params = json.loads(json_str)\n",
    "\n",
    "# Convert lists back to numpy arrays where needed\n",
    "def restore_numpy_arrays(params):\n",
    "    restored = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            restored[key] = {k: np.array(v) if isinstance(v, list) else v for k, v in value.items()}\n",
    "        elif isinstance(value, list) and key in ['bin_edges', 'bin_representatives']:\n",
    "            restored[key] = np.array(value)\n",
    "        else:\n",
    "            restored[key] = value\n",
    "    return restored\n",
    "\n",
    "restored_params = restore_numpy_arrays(loaded_params)\n",
    "json_binning = EqualWidthBinning(**restored_params)\n",
    "\n",
    "# Test JSON-restored binning\n",
    "json_result = json_binning.transform(X_pandas.iloc[:3])\n",
    "print(f\"JSON-restored binning works: {np.allclose(original_result[:3], json_result)}\")\n",
    "\n",
    "print(\"\\n2. Pickle Serialization (for Python objects):\")\n",
    "\n",
    "# Pickle serialization (preserves exact object state)\n",
    "import io\n",
    "buffer = io.BytesIO()\n",
    "pickle.dump(original_binning, buffer)\n",
    "buffer.seek(0)\n",
    "pickled_binning = pickle.load(buffer)\n",
    "\n",
    "# Test pickled binning\n",
    "pickle_result = pickled_binning.transform(X_pandas.iloc[:3])\n",
    "print(f\"Pickled binning works: {np.allclose(original_result[:3], pickle_result)}\")\n",
    "print(f\"Pickled binning is fitted: {pickled_binning._fitted}\")\n",
    "\n",
    "print(\"\\n3. Custom Serialization Function:\")\n",
    "\n",
    "def serialize_binning(binning_obj):\n",
    "    \"\"\"Custom serialization function for binning objects.\"\"\"\n",
    "    return {\n",
    "        'class_name': binning_obj.__class__.__name__,\n",
    "        'params': binning_obj.get_params(),\n",
    "        'fitted': binning_obj._fitted,\n",
    "        'version': '1.0'  # For version compatibility\n",
    "    }\n",
    "\n",
    "def deserialize_binning(serialized_data):\n",
    "    \"\"\"Custom deserialization function for binning objects.\"\"\"\n",
    "    if serialized_data['class_name'] != 'EqualWidthBinning':\n",
    "        raise ValueError(f\"Unsupported class: {serialized_data['class_name']}\")\n",
    "    \n",
    "    return EqualWidthBinning(**serialized_data['params'])\n",
    "\n",
    "# Test custom serialization\n",
    "serialized = serialize_binning(original_binning)\n",
    "deserialized_binning = deserialize_binning(serialized)\n",
    "\n",
    "custom_result = deserialized_binning.transform(X_pandas.iloc[:3])\n",
    "print(f\"Custom serialization works: {np.allclose(original_result[:3], custom_result)}\")\n",
    "\n",
    "print(\"\\n4. Summary:\")\n",
    "print(\"✓ Parameter transfer enables object reconstruction\")\n",
    "print(\"✓ JSON serialization works for configuration storage\") \n",
    "print(\"✓ Pickle serialization preserves complete object state\")\n",
    "print(\"✓ Custom serialization allows version control and validation\")\n",
    "print(\"✓ All methods produce identical transformation results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a145f2",
   "metadata": {},
   "source": [
    "## 5. Advanced Features and Edge Cases\n",
    "\n",
    "Let's explore some advanced features and how EqualWidthBinning handles various edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dedc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Handling Missing Values ===\n",
      "Original data with NaN count per column:\n",
      "normal_feature         10\n",
      "exponential_feature     5\n",
      "uniform_feature         0\n",
      "bimodal_feature         0\n",
      "dtype: int64\n",
      "\n",
      "Binned data with NaN count per column:\n",
      "normal_feature: 0\n",
      "exponential_feature: 0\n",
      "uniform_feature: 0\n",
      "bimodal_feature: 0\n",
      "\n",
      "Binned data sample with NaN:\n",
      "[[-1  0  0  0]\n",
      " [-1  0  2  0]\n",
      " [-1  0  0  0]\n",
      " [-1  0  1  0]\n",
      " [-1  0  2  0]]\n",
      "\n",
      "=== Out-of-bounds Values ===\n",
      "Training data range:\n",
      "  normal_feature: [1.38, 107.79]\n",
      "  exponential_feature: [0.01, 12.81]\n",
      "  uniform_feature: [0.00, 99.78]\n",
      "  bimodal_feature: [5.04, 94.34]\n",
      "\n",
      "Full data range:\n",
      "  normal_feature: [1.38, 107.79]\n",
      "  exponential_feature: [0.01, 14.88]\n",
      "  uniform_feature: [0.00, 99.78]\n",
      "  bimodal_feature: [5.04, 94.34]\n",
      "\n",
      "Binned full data range:\n",
      "  normal_feature: [0.00, 4.00]\n",
      "  exponential_feature: [0.00, 4.00]\n",
      "  uniform_feature: [0.00, 4.00]\n",
      "  bimodal_feature: [0.00, 4.00]\n",
      "\n",
      "=== Single Column Processing ===\n",
      "Single column binning:\n",
      "Bin edges: [np.float64(1.380989898963911), np.float64(19.115986975773392), np.float64(36.850984052582874), np.float64(54.58598112939236), np.float64(72.32097820620183), np.float64(90.05597528301132), np.float64(107.79097235982081)]\n",
      "Unique bin values: [0 1 2 3 4 5]\n",
      "\n",
      "=== Reproducibility ===\n",
      "Bin edges are identical across instances: True\n",
      "Transform results are identical: True\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Handling Missing Values\n",
    "print(\"=== Handling Missing Values ===\")\n",
    "\n",
    "# Create data with missing values\n",
    "data_with_na = X_pandas.copy()\n",
    "data_with_na.iloc[10:20, 0] = np.nan\n",
    "data_with_na.iloc[50:55, 1] = np.nan\n",
    "\n",
    "binning_na = EqualWidthBinning(n_bins=4)\n",
    "result_with_na = binning_na.fit_transform(data_with_na)\n",
    "\n",
    "print(f\"Original data with NaN count per column:\")\n",
    "print(data_with_na.isnull().sum())\n",
    "print(f\"\\nBinned data with NaN count per column:\")\n",
    "# Handle both DataFrame and array outputs\n",
    "if isinstance(result_with_na, pd.DataFrame):\n",
    "    print(result_with_na.isnull().sum())\n",
    "    print(f\"\\nBinned data sample with NaN:\")\n",
    "    print(result_with_na.iloc[10:15])\n",
    "else:\n",
    "    # For numpy arrays, count NaN values\n",
    "    nan_counts = [np.isnan(result_with_na[:, i]).sum() for i in range(result_with_na.shape[1])]\n",
    "    for i, count in enumerate(nan_counts):\n",
    "        col_name = data_with_na.columns[i] if i < len(data_with_na.columns) else f\"col_{i}\"\n",
    "        print(f\"{col_name}: {count}\")\n",
    "    print(f\"\\nBinned data sample with NaN:\")\n",
    "    print(result_with_na[10:15])\n",
    "\n",
    "# 5.2 Out-of-bounds values during transform\n",
    "print(\"\\n=== Out-of-bounds Values ===\")\n",
    "\n",
    "# Fit on a subset\n",
    "subset_data = X_pandas.iloc[200:800]  # Middle portion\n",
    "binning_subset = EqualWidthBinning(n_bins=5)\n",
    "binning_subset.fit(subset_data)\n",
    "\n",
    "print(f\"Training data range:\")\n",
    "for col in subset_data.columns:\n",
    "    print(f\"  {col}: [{subset_data[col].min():.2f}, {subset_data[col].max():.2f}]\")\n",
    "\n",
    "# Transform full data (includes out-of-bounds values)\n",
    "full_result = binning_subset.transform(X_pandas)\n",
    "\n",
    "print(f\"\\nFull data range:\")\n",
    "for col in X_pandas.columns:\n",
    "    print(f\"  {col}: [{X_pandas[col].min():.2f}, {X_pandas[col].max():.2f}]\")\n",
    "\n",
    "print(f\"\\nBinned full data range:\")\n",
    "if isinstance(full_result, pd.DataFrame):\n",
    "    for col in full_result.columns:\n",
    "        print(f\"  {col}: [{full_result[col].min():.2f}, {full_result[col].max():.2f}]\")\n",
    "else:\n",
    "    for i in range(full_result.shape[1]):\n",
    "        col_name = X_pandas.columns[i] if i < len(X_pandas.columns) else f\"col_{i}\"\n",
    "        print(f\"  {col_name}: [{full_result[:, i].min():.2f}, {full_result[:, i].max():.2f}]\")\n",
    "\n",
    "# 5.3 Single column vs multi-column\n",
    "print(\"\\n=== Single Column Processing ===\")\n",
    "\n",
    "# Process just one column\n",
    "single_col_binning = EqualWidthBinning(n_bins=6)\n",
    "single_col_result = single_col_binning.fit_transform(X_pandas[['normal_feature']])\n",
    "\n",
    "print(f\"Single column binning:\")\n",
    "print(f\"Bin edges: {list(single_col_binning._bin_edges.values())[0]}\")\n",
    "# Handle both DataFrame and array outputs\n",
    "if isinstance(single_col_result, pd.DataFrame):\n",
    "    unique_values = np.unique(single_col_result.iloc[:, 0])\n",
    "else:\n",
    "    unique_values = np.unique(single_col_result[:, 0])\n",
    "print(f\"Unique bin values: {unique_values}\")\n",
    "\n",
    "# 5.4 Reproducibility\n",
    "print(\"\\n=== Reproducibility ===\")\n",
    "\n",
    "# Create identical binning instances\n",
    "binning1 = EqualWidthBinning(n_bins=5)\n",
    "binning2 = EqualWidthBinning(n_bins=5)\n",
    "\n",
    "# Fit on same data\n",
    "binning1.fit(X_pandas)\n",
    "binning2.fit(X_pandas)\n",
    "\n",
    "# Check if bin edges are identical\n",
    "edges_identical = all(\n",
    "    np.allclose(edges1, edges2) \n",
    "    for edges1, edges2 in zip(binning1._bin_edges.values(), binning2._bin_edges.values())\n",
    ")\n",
    "\n",
    "print(f\"Bin edges are identical across instances: {edges_identical}\")\n",
    "\n",
    "# Transform and check results\n",
    "result1 = binning1.transform(X_pandas.iloc[:10])\n",
    "result2 = binning2.transform(X_pandas.iloc[:10])\n",
    "\n",
    "# Handle different output types for comparison\n",
    "if isinstance(result1, pd.DataFrame) and isinstance(result2, pd.DataFrame):\n",
    "    results_identical = np.allclose(result1.values, result2.values)\n",
    "elif isinstance(result1, np.ndarray) and isinstance(result2, np.ndarray):\n",
    "    results_identical = np.allclose(result1, result2)\n",
    "else:\n",
    "    # Mixed types - convert to arrays for comparison\n",
    "    arr1 = result1.values if hasattr(result1, 'values') else result1\n",
    "    arr2 = result2.values if hasattr(result2, 'values') else result2\n",
    "    results_identical = np.allclose(arr1, arr2)\n",
    "\n",
    "print(f\"Transform results are identical: {results_identical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a3a01",
   "metadata": {},
   "source": [
    "## 6. Summary and Best Practices\n",
    "\n",
    "This notebook demonstrated the comprehensive functionality of EqualWidthBinning:\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "1. **Multi-format Support**: Works seamlessly with NumPy arrays, Pandas DataFrames, and Polars DataFrames\n",
    "2. **Sklearn Integration**: Full compatibility with scikit-learn pipelines and workflows\n",
    "3. **Serialization**: Complete parameter transfer allowing model recreation without refitting\n",
    "4. **Edge Case Handling**: Robust handling of missing values, constant data, and out-of-bounds values\n",
    "5. **Reproducibility**: Consistent results across identical configurations\n",
    "\n",
    "### Best Practices:\n",
    "- Always use separate `fit()` and `transform()` for train/test splits to avoid data leakage\n",
    "- Leverage `get_params()` and `set_params()` for model serialization and deployment\n",
    "- Use pipeline integration for clean, maintainable ML workflows\n",
    "- Handle edge cases (missing values, constant features) appropriately in your preprocessing pipeline\n",
    "\n",
    "### Typical Workflow:\n",
    "```python\n",
    "# 1. Initialize\n",
    "binning = EqualWidthBinning(n_bins=5)\n",
    "\n",
    "# 2. Fit on training data\n",
    "binning.fit(X_train)\n",
    "\n",
    "# 3. Transform both train and test\n",
    "X_train_binned = binning.transform(X_train)\n",
    "X_test_binned = binning.transform(X_test)\n",
    "\n",
    "# 4. Save parameters for later use\n",
    "params = binning.get_params()\n",
    "\n",
    "# 5. Recreate identical binning later\n",
    "new_binning = EqualWidthBinning().set_params(**params)\n",
    "X_new_binned = new_binning.transform(X_new)  # No fit required!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
