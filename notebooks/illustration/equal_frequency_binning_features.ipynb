{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe327e01",
   "metadata": {},
   "source": [
    "# EqualFrequencyBinning: Comprehensive Feature Demonstration\n",
    "\n",
    "This notebook provides a comprehensive demonstration of the `EqualFrequencyBinning` class from the binlearn library, showcasing all its features and capabilities.\n",
    "\n",
    "## Core Features:\n",
    "- **Various Input/Output Formats**: Working with numpy arrays and pandas DataFrames\n",
    "- **Sklearn Pipeline Integration**: Seamless integration with scikit-learn workflows\n",
    "- **Joint vs Per-Column Fitting**: Comparing different fitting strategies\n",
    "- **Skewed Data Handling**: Demonstrating advantages over equal-width binning\n",
    "- **Quantile-Based Binning**: Creating bins with approximately equal sample sizes\n",
    "\n",
    "## Advanced Features:\n",
    "- **Robust Outlier Handling**: Better performance with extreme values\n",
    "- **Edge Case Handling**: Dealing with duplicate values and small datasets\n",
    "- **Performance Analysis**: Speed and memory considerations for sorting-based approach\n",
    "- **Visual Comparisons**: Enhanced plotting showing balanced bin populations\n",
    "\n",
    "## Overview\n",
    "\n",
    "`EqualFrequencyBinning` is a quantile-based binning method that divides the data so that each bin contains approximately the same number of samples (equal frequency). This makes it particularly effective for handling skewed distributions and ensuring balanced bin populations, unlike equal width binning which can create unbalanced bins with skewed data.\n",
    "\n",
    "### Key Advantages:\n",
    "- **Balanced Bins**: Each bin contains roughly the same number of samples\n",
    "- **Skewness Robust**: Handles highly skewed distributions effectively\n",
    "- **Outlier Resilient**: Extreme values don't dominate bin boundaries\n",
    "- **Predictable Sample Sizes**: Ensures adequate representation in each bin\n",
    "\n",
    "### When to Use EqualFrequencyBinning:\n",
    "‚úÖ **Good for**: Skewed data, ensuring balanced representation, outlier-heavy datasets  \n",
    "‚ö†Ô∏è **Caution with**: Categorical/discrete data with many duplicates  \n",
    "‚ùå **Avoid for**: When interpretable bin boundaries are critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822de9c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import all necessary libraries for our comprehensive demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "NumPy version: 2.3.2\n",
      "Pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports for pipeline integration\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binlearn imports - focusing on EqualFrequencyBinning but also import EqualWidthBinning for comparison\n",
    "from binlearn.methods import EqualFrequencyBinning, EqualWidthBinning\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn available for pipeline integration\")\n",
    "print(f\"EqualFrequencyBinning ready for comprehensive demonstration\")\n",
    "print(f\"EqualWidthBinning available for comparison purposes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13d95b",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Sample Data\n",
    "\n",
    "Let's create various types of sample datasets to demonstrate EqualFrequencyBinning's advantages, particularly with skewed distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c4393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating NumPy array data with skewed distributions...\n",
      "NumPy array shape: (200, 3)\n",
      "NumPy array type: <class 'numpy.ndarray'>\n",
      "\n",
      "üìä Creating Pandas DataFrame with skewed data...\n",
      "DataFrame shape: (200, 4)\n",
      "DataFrame columns: ['age', 'income', 'score', 'wait_time']\n",
      "\n",
      "üìä Creating Pandas Series with extreme skew...\n",
      "Series shape: (200,)\n",
      "Series name: pareto_feature\n",
      "\n",
      "üìà Data Statistics (notice the skewness):\n",
      "\n",
      "DataFrame describe:\n",
      "          age     income   score  wait_time\n",
      "count  200.00     200.00  200.00     200.00\n",
      "mean    28.52   29659.90   28.29       2.97\n",
      "std      7.20   24923.83   15.66       2.74\n",
      "min     18.44    2395.03    0.72       0.00\n",
      "25%     23.08   12451.15   16.77       0.97\n",
      "50%     26.71   22103.67   26.46       2.15\n",
      "75%     31.85   39382.04   38.14       3.99\n",
      "max     55.01  138735.99   73.81      12.57\n",
      "\n",
      "Series describe:\n",
      "count    200.00\n",
      "mean      26.66\n",
      "std       50.73\n",
      "min        0.02\n",
      "25%        2.95\n",
      "50%        8.75\n",
      "75%       23.49\n",
      "max      424.91\n",
      "Name: pareto_feature, dtype: float64\n",
      "\n",
      "üìê Skewness values (> 1 indicates high skewness):\n",
      "  age: 1.14\n",
      "  income: 1.93\n",
      "  score: 0.48\n",
      "  wait_time: 1.43\n",
      "  pareto_feature: 4.07\n"
     ]
    }
   ],
   "source": [
    "# Create diverse datasets emphasizing scenarios where EqualFrequencyBinning excels\n",
    "print(\"üìä Creating Datasets Showcasing EqualFrequencyBinning Advantages\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "# 1. Highly skewed distributions (where EqualFrequencyBinning shines)\n",
    "print(\"\\nüéØ Dataset 1: Highly Skewed Distributions\")\n",
    "data_skewed = pd.DataFrame({\n",
    "    'exponential': np.random.exponential(2, n_samples),           # Right-skewed\n",
    "    'power_law': np.random.pareto(1.5, n_samples),               # Heavy-tailed\n",
    "    'log_normal': np.random.lognormal(0, 1, n_samples),          # Log-normal\n",
    "    'chi_squared': np.random.chisquare(2, n_samples)             # Chi-squared\n",
    "})\n",
    "\n",
    "print(f\"Shape: {data_skewed.shape}\")\n",
    "print(\"Columns: exponential, power_law, log_normal, chi_squared\")\n",
    "print(\"All distributions are highly right-skewed\")\n",
    "\n",
    "# 2. Outlier-heavy data\n",
    "print(\"\\nüéØ Dataset 2: Outlier-Heavy Data\")\n",
    "# Base normal data with extreme outliers\n",
    "base_data = np.random.normal(0, 1, n_samples)\n",
    "outlier_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
    "base_data[outlier_indices] = np.random.choice([-50, -30, 30, 50, 100], size=len(outlier_indices))\n",
    "\n",
    "data_outliers = pd.DataFrame({\n",
    "    'with_outliers': base_data,\n",
    "    'moderate_outliers': np.concatenate([\n",
    "        np.random.normal(0, 1, int(n_samples * 0.9)),\n",
    "        np.random.uniform(-10, 10, int(n_samples * 0.1))\n",
    "    ])\n",
    "})\n",
    "\n",
    "print(f\"Shape: {data_outliers.shape}\")\n",
    "print(f\"Outlier percentage: ~5% extreme outliers\")\n",
    "\n",
    "# 3. Mixed distribution dataset (for general testing)\n",
    "print(\"\\nüéØ Dataset 3: Mixed Distributions\")\n",
    "data_mixed = pd.DataFrame({\n",
    "    'uniform': np.random.uniform(0, 100, n_samples),\n",
    "    'normal': np.random.normal(50, 15, n_samples),\n",
    "    'skewed': np.random.exponential(2, n_samples),\n",
    "    'bimodal': np.concatenate([np.random.normal(25, 5, n_samples//2), \n",
    "                              np.random.normal(75, 5, n_samples//2)])\n",
    "})\n",
    "\n",
    "print(f\"Shape: {data_mixed.shape}\")\n",
    "\n",
    "# 4. Classification dataset (for pipeline testing)\n",
    "print(\"\\nüéØ Dataset 4: Classification Data\")\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=n_samples, \n",
    "    n_features=4, \n",
    "    n_informative=3,\n",
    "    n_redundant=1,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "data_classification = pd.DataFrame(X_class, columns=['feature_1', 'feature_2', 'feature_3', 'feature_4'])\n",
    "data_classification['target'] = y_class\n",
    "\n",
    "print(f\"Shape: {data_classification.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y_class)}\")\n",
    "\n",
    "# Display comprehensive statistics\n",
    "print(\"\\nüìà Dataset Statistics Summary:\")\n",
    "print(\"\\n1. Highly Skewed Data:\")\n",
    "print(data_skewed.describe().round(3))\n",
    "print(\"\\nSkewness values:\")\n",
    "for col in data_skewed.columns:\n",
    "    skewness = data_skewed[col].skew()\n",
    "    print(f\"   {col}: {skewness:.2f} ({'highly skewed' if abs(skewness) > 2 else 'moderately skewed' if abs(skewness) > 1 else 'roughly symmetric'})\")\n",
    "\n",
    "print(\"\\n2. Outlier-Heavy Data:\")\n",
    "print(data_outliers.describe().round(3))\n",
    "\n",
    "print(\"\\n3. Mixed Distributions:\")\n",
    "print(data_mixed.describe().round(3))\n",
    "\n",
    "print(\"\\n4. Classification Features:\")\n",
    "print(data_classification.iloc[:, :-1].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the datasets to understand distribution characteristics\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle('Data Distribution Characteristics - Where EqualFrequencyBinning Excels', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot skewed distributions\n",
    "for i, col in enumerate(data_skewed.columns):\n",
    "    ax = axes[0, i % 2] if i < 2 else axes[1, i % 2]\n",
    "    data_skewed[col].hist(bins=30, alpha=0.7, ax=ax, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'{col.title().replace(\"_\", \" \")} Distribution\\n(Skewness: {data_skewed[col].skew():.2f})')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "# Plot outlier-heavy data\n",
    "axes[1, 0].clear()\n",
    "data_outliers['with_outliers'].hist(bins=50, alpha=0.7, ax=axes[1, 0], color='lightcoral', edgecolor='black')\n",
    "axes[1, 0].set_title('Data with Extreme Outliers')\n",
    "axes[1, 0].set_xlabel('Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot mixed distributions comparison\n",
    "axes[1, 1].clear()\n",
    "for col in ['uniform', 'skewed']:\n",
    "    data_mixed[col].hist(bins=20, alpha=0.6, ax=axes[1, 1], label=col.title())\n",
    "axes[1, 1].set_title('Uniform vs Skewed Distribution')\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Plot classification features\n",
    "axes[2, 0].clear()\n",
    "axes[2, 1].clear()\n",
    "data_classification['feature_1'].hist(bins=20, alpha=0.7, ax=axes[2, 0], color='lightgreen', edgecolor='black')\n",
    "axes[2, 0].set_title('Classification Feature 1')\n",
    "axes[2, 0].set_xlabel('Value')\n",
    "axes[2, 0].set_ylabel('Frequency')\n",
    "\n",
    "data_classification['feature_2'].hist(bins=20, alpha=0.7, ax=axes[2, 1], color='lightgreen', edgecolor='black')\n",
    "axes[2, 1].set_title('Classification Feature 2')\n",
    "axes[2, 1].set_xlabel('Value')\n",
    "axes[2, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"‚Ä¢ Exponential and power-law distributions show extreme right skewness\")\n",
    "print(\"‚Ä¢ Outlier-heavy data has most values clustered with extreme outliers\")\n",
    "print(\"‚Ä¢ EqualFrequencyBinning will handle these challenging cases effectively\")\n",
    "print(\"‚Ä¢ Traditional fixed-width binning would struggle with these distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a88372",
   "metadata": {},
   "source": [
    "## üéØ EqualFrequencyBinning Core Functionality\n",
    "\n",
    "EqualFrequencyBinning creates bins with approximately equal number of observations, making it particularly effective for skewed distributions and outlier-heavy data. Unlike equal-width binning, it adapts to the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44219982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate EqualFrequencyBinning on highly skewed data\n",
    "print(\"üî¨ EqualFrequencyBinning Demonstration on Skewed Data\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Initialize EqualFrequencyBinning with different bin counts\n",
    "binning_5 = EqualFrequencyBinning(n_bins=5)\n",
    "binning_10 = EqualFrequencyBinning(n_bins=10)\n",
    "\n",
    "# Focus on exponential data (highly right-skewed)\n",
    "exponential_data = data_skewed[['exponential']].copy()\n",
    "\n",
    "print(f\"\\nüìä Original Data Statistics:\")\n",
    "print(f\"   Mean: {exponential_data['exponential'].mean():.3f}\")\n",
    "print(f\"   Median: {exponential_data['exponential'].median():.3f}\")\n",
    "print(f\"   Std: {exponential_data['exponential'].std():.3f}\")\n",
    "print(f\"   Skewness: {exponential_data['exponential'].skew():.3f}\")\n",
    "print(f\"   Min: {exponential_data['exponential'].min():.3f}\")\n",
    "print(f\"   Max: {exponential_data['exponential'].max():.3f}\")\n",
    "\n",
    "# Fit and transform with different bin counts\n",
    "print(f\"\\nüéØ EqualFrequencyBinning Results:\")\n",
    "\n",
    "for n_bins, binning in [(5, binning_5), (10, binning_10)]:\n",
    "    print(f\"\\n--- {n_bins} Bins ---\")\n",
    "    \n",
    "    # Fit and transform\n",
    "    binned_data = binning.fit_transform(exponential_data)\n",
    "    \n",
    "    # Get bin information\n",
    "    bin_edges = binning.bins_[0]  # For single column\n",
    "    print(f\"Bin edges: {[f'{edge:.3f}' for edge in bin_edges]}\")\n",
    "    \n",
    "    # Analyze frequency distribution\n",
    "    unique_bins, counts = np.unique(binned_data.iloc[:, 0], return_counts=True)\n",
    "    print(f\"Bin frequencies: {counts}\")\n",
    "    print(f\"Frequency balance (std): {np.std(counts):.2f}\")\n",
    "    \n",
    "    # Show bin assignments for different quantiles\n",
    "    quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "    print(\"Value ‚Üí Bin mapping for quantiles:\")\n",
    "    for q in quantiles:\n",
    "        value = exponential_data['exponential'].quantile(q)\n",
    "        bin_idx = binned_data.iloc[exponential_data['exponential'].argsort()[int(q * len(exponential_data))], 0]\n",
    "        print(f\"   {q*100:2.0f}th percentile ({value:.3f}) ‚Üí Bin {bin_idx}\")\n",
    "\n",
    "# Visualize the binning results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('EqualFrequencyBinning: Handling Skewed Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original distribution\n",
    "axes[0, 0].hist(exponential_data['exponential'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Original Exponential Distribution\\n(Highly Right-Skewed)')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Equal-frequency binning visualization\n",
    "binned_5 = binning_5.fit_transform(exponential_data)\n",
    "axes[0, 1].hist(binned_5.iloc[:, 0], bins=5, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_title('After EqualFrequencyBinning (5 bins)\\nBalanced Frequencies')\n",
    "axes[0, 1].set_xlabel('Bin Number')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Compare with EqualWidthBinning for contrast\n",
    "equal_width = EqualWidthBinning(n_bins=5)\n",
    "width_binned = equal_width.fit_transform(exponential_data)\n",
    "axes[1, 0].hist(width_binned.iloc[:, 0], bins=5, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('EqualWidthBinning Comparison\\n(Unbalanced due to skewness)')\n",
    "axes[1, 0].set_xlabel('Bin Number')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Bin boundaries visualization\n",
    "bin_edges_freq = binning_5.bins_[0]\n",
    "bin_edges_width = equal_width.bins_[0]\n",
    "\n",
    "axes[1, 1].hist(exponential_data['exponential'], bins=30, alpha=0.3, color='gray', label='Original data')\n",
    "for i, edge in enumerate(bin_edges_freq[1:-1]):\n",
    "    axes[1, 1].axvline(edge, color='red', linestyle='--', alpha=0.7, \n",
    "                      label='Freq. edges' if i == 0 else \"\")\n",
    "for i, edge in enumerate(bin_edges_width[1:-1]):\n",
    "    axes[1, 1].axvline(edge, color='green', linestyle=':', alpha=0.7,\n",
    "                      label='Width edges' if i == 0 else \"\")\n",
    "\n",
    "axes[1, 1].set_title('Bin Boundaries Comparison')\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"‚Ä¢ EqualFrequencyBinning adapts to data distribution\")\n",
    "print(\"‚Ä¢ Each bin contains approximately the same number of observations\")\n",
    "print(\"‚Ä¢ Bin boundaries are data-driven, not predetermined\")\n",
    "print(\"‚Ä¢ Particularly effective for skewed and outlier-heavy data\")\n",
    "print(\"‚Ä¢ Maintains statistical balance across bins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b0ffe",
   "metadata": {},
   "source": [
    "## üîÑ Joint vs Per-Column EqualFrequencyBinning\n",
    "\n",
    "EqualFrequencyBinning can operate in two modes: per-column (independent) or joint (coordinated across features). This choice significantly impacts the binning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace26e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare joint vs per-column binning strategies\n",
    "print(\"üîÑ Joint vs Per-Column EqualFrequencyBinning Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use mixed distribution data for clear differences\n",
    "comparison_data = data_mixed[['uniform', 'skewed', 'normal']].copy()\n",
    "\n",
    "print(\"üìä Original Data Characteristics:\")\n",
    "print(comparison_data.describe().round(3))\n",
    "print(\"\\nSkewness values:\")\n",
    "for col in comparison_data.columns:\n",
    "    print(f\"   {col}: {comparison_data[col].skew():.3f}\")\n",
    "\n",
    "# Per-column binning (default)\n",
    "print(f\"\\nüéØ 1. Per-Column Binning (Independent)\")\n",
    "binning_per_col = EqualFrequencyBinning(n_bins=5)\n",
    "binned_per_col = binning_per_col.fit_transform(comparison_data)\n",
    "\n",
    "print(\"Bin edges per column:\")\n",
    "for i, col in enumerate(comparison_data.columns):\n",
    "    edges = binning_per_col.bins_[i]\n",
    "    print(f\"   {col}: {[f'{edge:.2f}' for edge in edges]}\")\n",
    "\n",
    "print(\"\\nFrequency distribution per column:\")\n",
    "for i, col in enumerate(comparison_data.columns):\n",
    "    unique_bins, counts = np.unique(binned_per_col.iloc[:, i], return_counts=True)\n",
    "    print(f\"   {col}: {counts} (std: {np.std(counts):.2f})\")\n",
    "\n",
    "# Joint binning\n",
    "print(f\"\\nüéØ 2. Joint Binning (Coordinated)\")\n",
    "binning_joint = EqualFrequencyBinning(n_bins=5, fitting_mode='joint')\n",
    "binned_joint = binning_joint.fit_transform(comparison_data)\n",
    "\n",
    "print(\"Bin edges per column (joint fitting):\")\n",
    "for i, col in enumerate(comparison_data.columns):\n",
    "    edges = binning_joint.bins_[i] if hasattr(binning_joint, 'bins_') and binning_joint.bins_ else [\"Joint mode - check implementation\"]\n",
    "    print(f\"   {col}: {edges}\")\n",
    "\n",
    "print(\"\\nFrequency distribution per column (joint):\")\n",
    "for i, col in enumerate(comparison_data.columns):\n",
    "    unique_bins, counts = np.unique(binned_joint.iloc[:, i], return_counts=True)\n",
    "    print(f\"   {col}: {counts} (std: {np.std(counts):.2f})\")\n",
    "\n",
    "# Analyze cross-column patterns\n",
    "print(f\"\\nüìà Cross-Column Pattern Analysis:\")\n",
    "\n",
    "def analyze_cross_patterns(data, name):\n",
    "    print(f\"\\n{name} Binning:\")\n",
    "    # Joint distribution analysis\n",
    "    joint_patterns = data.value_counts().head(10)\n",
    "    print(f\"   Top 10 bin combinations:\")\n",
    "    for pattern, count in joint_patterns.items():\n",
    "        print(f\"      {pattern}: {count} observations\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlation = data.corr()\n",
    "    print(f\"   Bin correlation matrix:\")\n",
    "    print(correlation.round(3))\n",
    "\n",
    "analyze_cross_patterns(binned_per_col, \"Per-Column\")\n",
    "analyze_cross_patterns(binned_joint, \"Joint\")\n",
    "\n",
    "# Visualize the differences\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Per-Column vs Joint EqualFrequencyBinning Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Per-column results\n",
    "for i, col in enumerate(comparison_data.columns):\n",
    "    axes[0, i].hist(binned_per_col.iloc[:, i], bins=5, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, i].set_title(f'Per-Column: {col.title()}\\n(Independent binning)')\n",
    "    axes[0, i].set_xlabel('Bin Number')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "\n",
    "# Joint results\n",
    "for i, col in enumerate(comparison_data.columns):\n",
    "    axes[1, i].hist(binned_joint.iloc[:, i], bins=5, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1, i].set_title(f'Joint: {col.title()}\\n(Coordinated binning)')\n",
    "    axes[1, i].set_xlabel('Bin Number')\n",
    "    axes[1, i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed comparison table\n",
    "print(f\"\\nüìã Detailed Comparison Summary:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Frequency Balance', 'Cross-Column Coordination', 'Individual Optimality', 'Use Case'],\n",
    "    'Per-Column': [\n",
    "        'Optimal per feature',\n",
    "        'Independent',\n",
    "        'High',\n",
    "        'Feature-specific analysis'\n",
    "    ],\n",
    "    'Joint': [\n",
    "        'Globally balanced',\n",
    "        'Coordinated',\n",
    "        'Moderate',\n",
    "        'Multi-variate analysis'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° Practical Guidelines:\")\n",
    "print(\"‚Ä¢ Per-column: Use when features have different scales/distributions\")\n",
    "print(\"‚Ä¢ Per-column: Better for feature-specific analysis\")\n",
    "print(\"‚Ä¢ Joint: Use when maintaining cross-feature relationships is important\")\n",
    "print(\"‚Ä¢ Joint: Better for multivariate pattern discovery\")\n",
    "print(\"‚Ä¢ EqualFrequencyBinning handles skewness well in both modes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e916b67",
   "metadata": {},
   "source": [
    "## üîó Pipeline Integration with Scikit-learn\n",
    "\n",
    "EqualFrequencyBinning integrates seamlessly with scikit-learn pipelines, making it easy to incorporate into machine learning workflows, especially with skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate pipeline integration with EqualFrequencyBinning\n",
    "print(\"üîó EqualFrequencyBinning in Machine Learning Pipelines\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Prepare classification data\n",
    "X = data_classification.iloc[:, :-1]  # Features\n",
    "y = data_classification['target']     # Target\n",
    "\n",
    "print(f\"üìä Dataset Info:\")\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Target distribution: {np.bincount(y)}\")\n",
    "print(f\"   Feature ranges: {X.min().round(2).values} to {X.max().round(2).values}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nüéØ Train/Test Split:\")\n",
    "print(f\"   Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create multiple pipelines for comparison\n",
    "pipelines = {\n",
    "    'No Binning': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'EqualFrequency (3 bins)': Pipeline([\n",
    "        ('binning', EqualFrequencyBinning(n_bins=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'EqualFrequency (5 bins)': Pipeline([\n",
    "        ('binning', EqualFrequencyBinning(n_bins=5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'EqualFrequency (10 bins)': Pipeline([\n",
    "        ('binning', EqualFrequencyBinning(n_bins=10)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'EqualWidth (5 bins)': Pipeline([\n",
    "        ('binning', EqualWidthBinning(n_bins=5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Evaluate pipelines with cross-validation\n",
    "print(f\"\\nüî¨ Cross-Validation Results (5-fold):\")\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'cv_scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"   {name:25s}: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Train final models and evaluate on test set\n",
    "print(f\"\\nüéØ Test Set Performance:\")\n",
    "test_results = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Fit and predict\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"   {name:25s}: Acc={accuracy:.4f}, Prec={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "# Detailed pipeline analysis\n",
    "print(f\"\\nüîç Pipeline Component Analysis:\")\n",
    "\n",
    "# Analyze feature transformations\n",
    "freq_pipeline = pipelines['EqualFrequency (5 bins)']\n",
    "freq_pipeline.fit(X_train, y_train)\n",
    "\n",
    "if 'binning' in freq_pipeline.named_steps:\n",
    "    binning_step = freq_pipeline.named_steps['binning']\n",
    "    print(f\"\\nEqualFrequencyBinning (5 bins) - Bin Information:\")\n",
    "    \n",
    "    for i, col in enumerate(X.columns):\n",
    "        edges = binning_step.bins_[i]\n",
    "        print(f\"   {col}: {len(edges)-1} bins with edges {[f'{e:.2f}' for e in edges]}\")\n",
    "        \n",
    "        # Show data distribution across bins\n",
    "        binned_feature = binning_step.transform(X_train)[:, i]\n",
    "        unique_bins, counts = np.unique(binned_feature, return_counts=True)\n",
    "        print(f\"           Frequencies: {counts} (balance std: {np.std(counts):.2f})\")\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Pipeline': list(results.keys()),\n",
    "    'CV_Mean': [results[name]['cv_mean'] for name in results.keys()],\n",
    "    'CV_Std': [results[name]['cv_std'] for name in results.keys()],\n",
    "    'Test_Accuracy': [test_results[name]['accuracy'] for name in results.keys()],\n",
    "    'Test_F1': [test_results[name]['f1'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Complete Results Summary:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Pipeline Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Cross-validation scores\n",
    "axes[0].bar(range(len(results)), [results[name]['cv_mean'] for name in results.keys()], \n",
    "           yerr=[results[name]['cv_std'] for name in results.keys()], \n",
    "           alpha=0.7, capsize=5, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Cross-Validation Accuracy')\n",
    "axes[0].set_xlabel('Pipeline')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xticks(range(len(results)))\n",
    "axes[0].set_xticklabels(list(results.keys()), rotation=45, ha='right')\n",
    "\n",
    "# Test set performance\n",
    "test_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "x = np.arange(len(pipelines))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(test_metrics):\n",
    "    values = [test_results[name][metric] for name in pipelines.keys()]\n",
    "    axes[1].bar(x + i*width, values, width, label=metric.title(), alpha=0.7)\n",
    "\n",
    "axes[1].set_title('Test Set Performance Metrics')\n",
    "axes[1].set_xlabel('Pipeline')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xticks(x + width * 1.5)\n",
    "axes[1].set_xticklabels(list(pipelines.keys()), rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Pipeline Integration Insights:\")\n",
    "print(\"‚Ä¢ EqualFrequencyBinning integrates seamlessly with scikit-learn\")\n",
    "print(\"‚Ä¢ Binning can improve or stabilize model performance\")\n",
    "print(\"‚Ä¢ Different bin counts offer trade-offs between complexity and smoothing\")\n",
    "print(\"‚Ä¢ Works well with standard preprocessing steps (scaling, etc.)\")\n",
    "print(\"‚Ä¢ Particularly valuable for handling skewed feature distributions\")\n",
    "\n",
    "# Show feature importance analysis if available\n",
    "if hasattr(freq_pipeline.named_steps['classifier'], 'coef_'):\n",
    "    print(f\"\\nüéØ Feature Importance Analysis (EqualFrequency 5 bins):\")\n",
    "    coefficients = freq_pipeline.named_steps['classifier'].coef_[0]\n",
    "    \n",
    "    # Create feature names for binned features\n",
    "    binned_feature_names = []\n",
    "    for col in X.columns:\n",
    "        binned_feature_names.append(f\"{col}_binned\")\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': binned_feature_names,\n",
    "        'Coefficient': coefficients,\n",
    "        'Abs_Coefficient': np.abs(coefficients)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(importance_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237a107",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Edge Cases and Robustness Testing\n",
    "\n",
    "EqualFrequencyBinning handles challenging data scenarios well, but understanding its behavior in edge cases is crucial for robust applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e10026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EqualFrequencyBinning robustness in edge cases\n",
    "print(\"‚ö†Ô∏è EqualFrequencyBinning Edge Cases and Robustness Testing\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Edge Case 1: Data with many duplicate values\n",
    "print(\"üîç Edge Case 1: Data with Many Duplicate Values\")\n",
    "duplicate_data = pd.DataFrame({\n",
    "    'many_duplicates': np.concatenate([\n",
    "        np.full(100, 1.0),    # 100 values of 1.0\n",
    "        np.full(80, 2.0),     # 80 values of 2.0\n",
    "        np.full(60, 3.0),     # 60 values of 3.0\n",
    "        np.full(40, 4.0),     # 40 values of 4.0\n",
    "        np.full(20, 5.0)      # 20 values of 5.0\n",
    "    ])\n",
    "})\n",
    "\n",
    "print(f\"Value counts:\")\n",
    "print(duplicate_data['many_duplicates'].value_counts().sort_index())\n",
    "\n",
    "try:\n",
    "    binning_duplicates = EqualFrequencyBinning(n_bins=5)\n",
    "    binned_duplicates = binning_duplicates.fit_transform(duplicate_data)\n",
    "    \n",
    "    print(f\"‚úÖ Binning successful!\")\n",
    "    print(f\"Bin edges: {[f'{edge:.1f}' for edge in binning_duplicates.bins_[0]]}\")\n",
    "    \n",
    "    unique_bins, counts = np.unique(binned_duplicates.iloc[:, 0], return_counts=True)\n",
    "    print(f\"Bin frequencies: {counts}\")\n",
    "    print(f\"Note: Perfect equal frequency impossible due to duplicate values\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Edge Case 2: Very small dataset\n",
    "print(f\"\\nüîç Edge Case 2: Very Small Dataset\")\n",
    "small_data = pd.DataFrame({\n",
    "    'small_sample': [1.0, 2.0, 3.0, 4.0, 5.0]  # Only 5 samples\n",
    "})\n",
    "\n",
    "try:\n",
    "    binning_small = EqualFrequencyBinning(n_bins=3)\n",
    "    binned_small = binning_small.fit_transform(small_data)\n",
    "    \n",
    "    print(f\"‚úÖ Small data binning successful!\")\n",
    "    print(f\"Original data: {small_data['small_sample'].values}\")\n",
    "    print(f\"Binned data: {binned_small.iloc[:, 0].values}\")\n",
    "    print(f\"Bin edges: {[f'{edge:.1f}' for edge in binning_small.bins_[0]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Edge Case 3: Data with extreme outliers\n",
    "print(f\"\\nüîç Edge Case 3: Data with Extreme Outliers\")\n",
    "outlier_data = pd.DataFrame({\n",
    "    'with_outliers': np.concatenate([\n",
    "        np.random.normal(0, 1, 95),    # 95 normal values\n",
    "        [1000, -1000, 5000, -2000, 10000]  # 5 extreme outliers\n",
    "    ])\n",
    "})\n",
    "\n",
    "print(f\"Data range: {outlier_data['with_outliers'].min():.1f} to {outlier_data['with_outliers'].max():.1f}\")\n",
    "print(f\"Standard deviation: {outlier_data['with_outliers'].std():.1f}\")\n",
    "\n",
    "try:\n",
    "    binning_outliers = EqualFrequencyBinning(n_bins=5)\n",
    "    binned_outliers = binning_outliers.fit_transform(outlier_data)\n",
    "    \n",
    "    print(f\"‚úÖ Outlier handling successful!\")\n",
    "    \n",
    "    # Show how outliers are distributed\n",
    "    unique_bins, counts = np.unique(binned_outliers.iloc[:, 0], return_counts=True)\n",
    "    print(f\"Bin frequencies: {counts}\")\n",
    "    \n",
    "    # Identify which bin contains outliers\n",
    "    extreme_values = outlier_data['with_outliers'] > 100\n",
    "    if extreme_values.any():\n",
    "        outlier_bins = binned_outliers.iloc[extreme_values, 0].unique()\n",
    "        print(f\"Outliers assigned to bins: {outlier_bins}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Edge Case 4: Constant data\n",
    "print(f\"\\nüîç Edge Case 4: Constant Data\")\n",
    "constant_data = pd.DataFrame({\n",
    "    'constant': np.full(100, 42.0)  # All values are 42.0\n",
    "})\n",
    "\n",
    "try:\n",
    "    binning_constant = EqualFrequencyBinning(n_bins=5)\n",
    "    binned_constant = binning_constant.fit_transform(constant_data)\n",
    "    \n",
    "    print(f\"‚úÖ Constant data handling successful!\")\n",
    "    print(f\"All values: {constant_data['constant'].iloc[0]}\")\n",
    "    print(f\"Bin assignments: {binned_constant.iloc[:, 0].unique()}\")\n",
    "    print(f\"Bin edges: {binning_constant.bins_[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Edge Case 5: Missing values\n",
    "print(f\"\\nüîç Edge Case 5: Data with Missing Values\")\n",
    "missing_data = pd.DataFrame({\n",
    "    'with_nan': np.concatenate([\n",
    "        np.random.normal(0, 1, 80),    # 80 normal values\n",
    "        [np.nan] * 20                   # 20 missing values\n",
    "    ])\n",
    "})\n",
    "\n",
    "print(f\"Missing values: {missing_data['with_nan'].isna().sum()}\")\n",
    "print(f\"Valid values: {missing_data['with_nan'].notna().sum()}\")\n",
    "\n",
    "try:\n",
    "    binning_missing = EqualFrequencyBinning(n_bins=5)\n",
    "    # Note: Most binning methods require handling NaN values first\n",
    "    clean_data = missing_data.dropna()\n",
    "    binned_missing = binning_missing.fit_transform(clean_data)\n",
    "    \n",
    "    print(f\"‚úÖ Missing value handling (after dropping NaN)!\")\n",
    "    print(f\"Processed {len(clean_data)} valid observations\")\n",
    "    \n",
    "    unique_bins, counts = np.unique(binned_missing.iloc[:, 0], return_counts=True)\n",
    "    print(f\"Bin frequencies: {counts}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Edge Case 6: More bins than unique values\n",
    "print(f\"\\nüîç Edge Case 6: More Bins than Unique Values\")\n",
    "limited_unique = pd.DataFrame({\n",
    "    'few_unique': [1, 1, 2, 2, 3, 3] * 10  # Only 3 unique values, 60 total\n",
    "})\n",
    "\n",
    "print(f\"Unique values: {sorted(limited_unique['few_unique'].unique())}\")\n",
    "print(f\"Total observations: {len(limited_unique)}\")\n",
    "\n",
    "try:\n",
    "    binning_limited = EqualFrequencyBinning(n_bins=10)  # More bins than unique values\n",
    "    binned_limited = binning_limited.fit_transform(limited_unique)\n",
    "    \n",
    "    print(f\"‚úÖ Limited unique values handling successful!\")\n",
    "    print(f\"Requested 10 bins, got {len(binning_limited.bins_[0])-1} effective bins\")\n",
    "    print(f\"Bin edges: {binning_limited.bins_[0]}\")\n",
    "    \n",
    "    unique_bins, counts = np.unique(binned_limited.iloc[:, 0], return_counts=True)\n",
    "    print(f\"Actual bins used: {len(unique_bins)}\")\n",
    "    print(f\"Bin frequencies: {counts}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Visualization of edge cases\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('EqualFrequencyBinning: Edge Cases Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot edge cases\n",
    "edge_cases = [\n",
    "    (duplicate_data['many_duplicates'], 'Many Duplicates'),\n",
    "    (outlier_data['with_outliers'], 'Extreme Outliers'),\n",
    "    (constant_data['constant'], 'Constant Data'),\n",
    "    (missing_data['with_nan'].dropna(), 'After Removing NaN'),\n",
    "    (limited_unique['few_unique'], 'Few Unique Values'),\n",
    "    (small_data['small_sample'], 'Very Small Dataset')\n",
    "]\n",
    "\n",
    "for i, (data, title) in enumerate(edge_cases):\n",
    "    row, col = i // 3, i % 3\n",
    "    axes[row, col].hist(data, bins=min(20, len(data.unique())), alpha=0.7, \n",
    "                       color='lightcoral', edgecolor='black')\n",
    "    axes[row, col].set_title(f'{title}\\n({len(data)} observations)')\n",
    "    axes[row, col].set_xlabel('Value')\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Edge Case Insights:\")\n",
    "print(\"‚Ä¢ EqualFrequencyBinning handles most edge cases gracefully\")\n",
    "print(\"‚Ä¢ Duplicate values prevent perfect equal frequencies\")\n",
    "print(\"‚Ä¢ Extreme outliers are naturally accommodated\")\n",
    "print(\"‚Ä¢ Constant data results in single bin assignment\")\n",
    "print(\"‚Ä¢ More bins than unique values are automatically reduced\")\n",
    "print(\"‚Ä¢ Missing values should be handled before binning\")\n",
    "print(\"‚Ä¢ Small datasets may not achieve ideal frequency distribution\")\n",
    "\n",
    "# Summary table of edge case handling\n",
    "edge_case_summary = pd.DataFrame({\n",
    "    'Edge Case': ['Many Duplicates', 'Small Dataset', 'Extreme Outliers', \n",
    "                  'Constant Data', 'Missing Values', 'Limited Unique Values'],\n",
    "    'Handling': ['Approximate frequencies', 'Works with constraints', 'Robust accommodation',\n",
    "                'Single bin assignment', 'Requires preprocessing', 'Auto bin reduction'],\n",
    "    'Recommendation': ['Use fewer bins', 'Consider bin count', 'EqualFreq advantage',\n",
    "                      'Check for constants', 'Impute or drop NaN', 'Reduce n_bins parameter']\n",
    "})\n",
    "\n",
    "print(f\"\\nüìã Edge Case Handling Summary:\")\n",
    "print(edge_case_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ee87f",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Analysis and Benchmarking\n",
    "\n",
    "Understanding the computational characteristics of EqualFrequencyBinning helps optimize its use in different scenarios and data sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a41c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking for EqualFrequencyBinning\n",
    "print(\"‚ö° EqualFrequencyBinning Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different data sizes and configurations\n",
    "test_sizes = [100, 1000, 10000, 50000]\n",
    "test_features = [1, 5, 10, 20]\n",
    "test_bins = [3, 5, 10, 20]\n",
    "\n",
    "# Performance tracking\n",
    "performance_results = []\n",
    "\n",
    "print(\"üî¨ Performance Testing Configuration:\")\n",
    "print(f\"   Data sizes: {test_sizes}\")\n",
    "print(f\"   Feature counts: {test_features}\")\n",
    "print(f\"   Bin counts: {test_bins}\")\n",
    "\n",
    "# Benchmark 1: Scaling with data size\n",
    "print(f\"\\nüìä Benchmark 1: Scaling with Data Size (5 features, 5 bins)\")\n",
    "size_performance = {}\n",
    "\n",
    "for size in test_sizes:\n",
    "    # Generate test data\n",
    "    np.random.seed(42)\n",
    "    test_data = pd.DataFrame({\n",
    "        f'feature_{i}': np.random.exponential(2, size) \n",
    "        for i in range(5)\n",
    "    })\n",
    "    \n",
    "    # Measure fitting time\n",
    "    binning = EqualFrequencyBinning(n_bins=5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    binning.fit(test_data)\n",
    "    fit_time = time.time() - start_time\n",
    "    \n",
    "    # Measure transform time\n",
    "    start_time = time.time()\n",
    "    transformed = binning.transform(test_data)\n",
    "    transform_time = time.time() - start_time\n",
    "    \n",
    "    # Measure fit_transform time\n",
    "    start_time = time.time()\n",
    "    binning_new = EqualFrequencyBinning(n_bins=5)\n",
    "    fit_transformed = binning_new.fit_transform(test_data)\n",
    "    fit_transform_time = time.time() - start_time\n",
    "    \n",
    "    size_performance[size] = {\n",
    "        'fit_time': fit_time,\n",
    "        'transform_time': transform_time,\n",
    "        'fit_transform_time': fit_transform_time,\n",
    "        'total_time': fit_time + transform_time\n",
    "    }\n",
    "    \n",
    "    print(f\"   {size:6d} samples: Fit={fit_time:.4f}s, Transform={transform_time:.4f}s, Total={fit_time + transform_time:.4f}s\")\n",
    "\n",
    "# Benchmark 2: Scaling with feature count\n",
    "print(f\"\\nüìä Benchmark 2: Scaling with Feature Count (10k samples, 5 bins)\")\n",
    "feature_performance = {}\n",
    "\n",
    "for n_features in test_features:\n",
    "    # Generate test data\n",
    "    np.random.seed(42)\n",
    "    test_data = pd.DataFrame({\n",
    "        f'feature_{i}': np.random.exponential(2, 10000) \n",
    "        for i in range(n_features)\n",
    "    })\n",
    "    \n",
    "    binning = EqualFrequencyBinning(n_bins=5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    binning.fit_transform(test_data)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    feature_performance[n_features] = total_time\n",
    "    print(f\"   {n_features:2d} features: {total_time:.4f}s ({total_time/n_features:.4f}s per feature)\")\n",
    "\n",
    "# Benchmark 3: Scaling with bin count\n",
    "print(f\"\\nüìä Benchmark 3: Scaling with Bin Count (10k samples, 5 features)\")\n",
    "bin_performance = {}\n",
    "\n",
    "# Generate consistent test data\n",
    "np.random.seed(42)\n",
    "test_data_bins = pd.DataFrame({\n",
    "    f'feature_{i}': np.random.exponential(2, 10000) \n",
    "    for i in range(5)\n",
    "})\n",
    "\n",
    "for n_bins in test_bins:\n",
    "    binning = EqualFrequencyBinning(n_bins=n_bins)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    binning.fit_transform(test_data_bins)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    bin_performance[n_bins] = total_time\n",
    "    print(f\"   {n_bins:2d} bins: {total_time:.4f}s\")\n",
    "\n",
    "# Memory usage analysis\n",
    "print(f\"\\nüíæ Memory Usage Analysis:\")\n",
    "\n",
    "def get_memory_usage(data, n_bins):\n",
    "    \"\"\"Estimate memory usage for binning operation\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    # Original data size\n",
    "    original_size = data.memory_usage(deep=True).sum()\n",
    "    \n",
    "    # Fit binning\n",
    "    binning = EqualFrequencyBinning(n_bins=n_bins)\n",
    "    binned_data = binning.fit_transform(data)\n",
    "    \n",
    "    # Binned data size\n",
    "    binned_size = binned_data.memory_usage(deep=True).sum()\n",
    "    \n",
    "    # Model size (approximate)\n",
    "    model_size = sys.getsizeof(binning.bins_) if hasattr(binning, 'bins_') else 1000\n",
    "    \n",
    "    return {\n",
    "        'original_mb': original_size / 1024 / 1024,\n",
    "        'binned_mb': binned_size / 1024 / 1024,\n",
    "        'model_mb': model_size / 1024 / 1024,\n",
    "        'compression_ratio': original_size / binned_size if binned_size > 0 else 1\n",
    "    }\n",
    "\n",
    "# Test memory usage on different sizes\n",
    "for size in [1000, 10000, 50000]:\n",
    "    test_data_memory = pd.DataFrame({\n",
    "        f'feature_{i}': np.random.exponential(2, size) \n",
    "        for i in range(5)\n",
    "    })\n",
    "    \n",
    "    memory_stats = get_memory_usage(test_data_memory, 5)\n",
    "    print(f\"   {size:6d} samples: Original={memory_stats['original_mb']:.2f}MB, \"\n",
    "          f\"Binned={memory_stats['binned_mb']:.2f}MB, \"\n",
    "          f\"Model={memory_stats['model_mb']:.3f}MB\")\n",
    "\n",
    "# Compare with other binning methods\n",
    "print(f\"\\nüèÅ Method Comparison (10k samples, 5 features, 5 bins):\")\n",
    "\n",
    "# Generate consistent test data\n",
    "np.random.seed(42)\n",
    "comparison_data = pd.DataFrame({\n",
    "    f'feature_{i}': np.random.exponential(2, 10000) \n",
    "    for i in range(5)\n",
    "})\n",
    "\n",
    "methods = {\n",
    "    'EqualFrequencyBinning': EqualFrequencyBinning(n_bins=5),\n",
    "    'EqualWidthBinning': EqualWidthBinning(n_bins=5)\n",
    "}\n",
    "\n",
    "method_times = {}\n",
    "for name, method in methods.items():\n",
    "    start_time = time.time()\n",
    "    result = method.fit_transform(comparison_data)\n",
    "    total_time = time.time() - start_time\n",
    "    method_times[name] = total_time\n",
    "    print(f\"   {name:20s}: {total_time:.4f}s\")\n",
    "\n",
    "# Visualize performance results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('EqualFrequencyBinning Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Data size scaling\n",
    "axes[0, 0].plot(list(size_performance.keys()), \n",
    "               [size_performance[size]['fit_transform_time'] for size in size_performance.keys()],\n",
    "               'o-', color='blue', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('Scaling with Data Size')\n",
    "axes[0, 0].set_xlabel('Number of Samples')\n",
    "axes[0, 0].set_ylabel('Time (seconds)')\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature count scaling\n",
    "axes[0, 1].plot(list(feature_performance.keys()), \n",
    "               list(feature_performance.values()),\n",
    "               'o-', color='green', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_title('Scaling with Feature Count')\n",
    "axes[0, 1].set_xlabel('Number of Features')\n",
    "axes[0, 1].set_ylabel('Time (seconds)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bin count scaling\n",
    "axes[1, 0].plot(list(bin_performance.keys()), \n",
    "               list(bin_performance.values()),\n",
    "               'o-', color='red', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_title('Scaling with Bin Count')\n",
    "axes[1, 0].set_xlabel('Number of Bins')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Method comparison\n",
    "method_names = list(method_times.keys())\n",
    "method_values = list(method_times.values())\n",
    "axes[1, 1].bar(method_names, method_values, color=['skyblue', 'lightcoral'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Method Comparison')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nüìã Performance Summary:\")\n",
    "performance_summary = pd.DataFrame({\n",
    "    'Metric': ['Linear scaling with data size', 'Linear scaling with features', \n",
    "               'Minimal impact of bin count', 'Memory efficiency', 'Comparison vs EqualWidth'],\n",
    "    'Result': [\n",
    "        f\"~{(size_performance[50000]['fit_transform_time']/size_performance[1000]['fit_transform_time']):.1f}x slower for 50x data\",\n",
    "        f\"~{(feature_performance[20]/feature_performance[1]):.1f}x slower for 20x features\",\n",
    "        f\"~{(bin_performance[20]/bin_performance[3]):.1f}x slower for 6.7x bins\",\n",
    "        \"Moderate memory overhead\",\n",
    "        f\"~{(method_times['EqualFrequencyBinning']/method_times['EqualWidthBinning']):.1f}x vs EqualWidth\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(performance_summary.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° Performance Insights:\")\n",
    "print(\"‚Ä¢ EqualFrequencyBinning scales linearly with data size and features\")\n",
    "print(\"‚Ä¢ Bin count has minimal impact on performance\")\n",
    "print(\"‚Ä¢ Performance is competitive with other binning methods\")\n",
    "print(\"‚Ä¢ Memory usage is reasonable for most applications\")\n",
    "print(\"‚Ä¢ Quantile calculation is the main computational bottleneck\")\n",
    "print(\"‚Ä¢ Consider data size when choosing bin count for optimal performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038f566",
   "metadata": {},
   "source": [
    "## üìã Practical Recommendations and Best Practices\n",
    "\n",
    "Based on our comprehensive analysis, here are practical guidelines for effectively using EqualFrequencyBinning in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bff7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical recommendations and decision framework\n",
    "print(\"üìã EqualFrequencyBinning: Practical Recommendations\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Decision framework for when to use EqualFrequencyBinning\n",
    "decision_framework = {\n",
    "    \"Ideal Use Cases\": [\n",
    "        \"Highly skewed distributions (skewness > 2)\",\n",
    "        \"Data with extreme outliers\",\n",
    "        \"Heavy-tailed distributions\",\n",
    "        \"When balanced sample sizes per bin are critical\",\n",
    "        \"Ordinal encoding for categorical features\",\n",
    "        \"Reducing impact of outliers on model training\"\n",
    "    ],\n",
    "    \n",
    "    \"Consider Alternatives When\": [\n",
    "        \"Data is approximately normal\",\n",
    "        \"Domain-specific bin boundaries are required\",\n",
    "        \"Interpretability of bin ranges is crucial\",\n",
    "        \"Many duplicate values exist\",\n",
    "        \"Real-time scoring with changing data distributions\"\n",
    "    ],\n",
    "    \n",
    "    \"Parameter Selection Guidelines\": [\n",
    "        \"Start with 3-5 bins for initial exploration\",\n",
    "        \"Use 5-10 bins for most practical applications\",\n",
    "        \"Consider 10+ bins only for large datasets (>10k samples)\",\n",
    "        \"Fewer bins for small datasets (<1k samples)\",\n",
    "        \"Test multiple bin counts via cross-validation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, recommendations in decision_framework.items():\n",
    "    print(f\"\\nüéØ {category}:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"   ‚Ä¢ {rec}\")\n",
    "\n",
    "# Practical workflow recommendation\n",
    "print(f\"\\nüîÑ Recommended Workflow:\")\n",
    "workflow_steps = [\n",
    "    \"1. Exploratory Data Analysis\",\n",
    "    \"   - Check distribution shape (skewness, kurtosis)\",\n",
    "    \"   - Identify outliers and extreme values\",\n",
    "    \"   - Assess unique value counts\",\n",
    "    \n",
    "    \"2. Method Selection\",\n",
    "    \"   - EqualFrequency for skewed data\",\n",
    "    \"   - EqualWidth for uniform/normal data\",\n",
    "    \"   - Manual binning for domain knowledge\",\n",
    "    \n",
    "    \"3. Parameter Tuning\",\n",
    "    \"   - Start with default bin count (5)\",\n",
    "    \"   - Test 3, 5, 10 bins via cross-validation\",\n",
    "    \"   - Consider data size constraints\",\n",
    "    \n",
    "    \"4. Validation and Testing\",\n",
    "    \"   - Check bin frequency balance\",\n",
    "    \"   - Validate on holdout data\",\n",
    "    \"   - Test edge cases (outliers, duplicates)\",\n",
    "    \n",
    "    \"5. Production Considerations\",\n",
    "    \"   - Save fitted binning models\",\n",
    "    \"   - Monitor data distribution drift\",\n",
    "    \"   - Plan for new data edge cases\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "# Create comprehensive comparison matrix\n",
    "print(f\"\\nüìä Method Comparison Matrix:\")\n",
    "\n",
    "comparison_matrix = pd.DataFrame({\n",
    "    'Characteristic': [\n",
    "        'Skewed Data Handling',\n",
    "        'Outlier Robustness', \n",
    "        'Interpretability',\n",
    "        'Computational Speed',\n",
    "        'Memory Efficiency',\n",
    "        'Parameter Sensitivity',\n",
    "        'Domain Flexibility',\n",
    "        'Statistical Balance'\n",
    "    ],\n",
    "    'EqualFrequency': [\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Moderate ‚≠ê‚≠ê',\n",
    "        'Good ‚≠ê‚≠ê‚≠ê',\n",
    "        'Good ‚≠ê‚≠ê‚≠ê',\n",
    "        'Low ‚≠ê‚≠ê‚≠ê',\n",
    "        'Moderate ‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê'\n",
    "    ],\n",
    "    'EqualWidth': [\n",
    "        'Poor ‚≠ê',\n",
    "        'Poor ‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Low ‚≠ê‚≠ê‚≠ê',\n",
    "        'High ‚≠ê‚≠ê‚≠ê',\n",
    "        'Poor ‚≠ê'\n",
    "    ],\n",
    "    'Manual': [\n",
    "        'Variable ‚≠ê‚≠ê',\n",
    "        'Variable ‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Variable ‚≠ê‚≠ê',\n",
    "        'Excellent ‚≠ê‚≠ê‚≠ê',\n",
    "        'Variable ‚≠ê‚≠ê'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_matrix.to_string(index=False))\n",
    "\n",
    "# Common pitfalls and solutions\n",
    "print(f\"\\n‚ö†Ô∏è Common Pitfalls and Solutions:\")\n",
    "\n",
    "pitfalls = {\n",
    "    \"Pitfall\": [\n",
    "        \"Using too many bins for small datasets\",\n",
    "        \"Ignoring duplicate values\",\n",
    "        \"Not handling missing values\",\n",
    "        \"Applying to already discretized data\",\n",
    "        \"Not saving fitted models\",\n",
    "        \"Ignoring data distribution changes\"\n",
    "    ],\n",
    "    \"Solution\": [\n",
    "        \"Use n_bins ‚â§ sqrt(sample_size)\",\n",
    "        \"Check unique value counts first\",\n",
    "        \"Impute or drop NaN before binning\", \n",
    "        \"Apply only to continuous features\",\n",
    "        \"Use joblib/pickle for model persistence\",\n",
    "        \"Monitor and retrain periodically\"\n",
    "    ],\n",
    "    \"Prevention\": [\n",
    "        \"Data size analysis\",\n",
    "        \"Exploratory data analysis\",\n",
    "        \"Preprocessing pipeline\",\n",
    "        \"Feature type validation\",\n",
    "        \"MLOps best practices\",\n",
    "        \"Data drift monitoring\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "pitfall_df = pd.DataFrame(pitfalls)\n",
    "print(pitfall_df.to_string(index=False))\n",
    "\n",
    "# Performance optimization tips\n",
    "print(f\"\\n‚ö° Performance Optimization Tips:\")\n",
    "\n",
    "optimization_tips = [\n",
    "    \"‚Ä¢ Use vectorized operations when possible\",\n",
    "    \"‚Ä¢ Consider parallel processing for multiple features\",\n",
    "    \"‚Ä¢ Cache fitted models for repeated use\",\n",
    "    \"‚Ä¢ Batch process large datasets\",\n",
    "    \"‚Ä¢ Use appropriate data types (float32 vs float64)\",\n",
    "    \"‚Ä¢ Profile memory usage for very large datasets\",\n",
    "    \"‚Ä¢ Consider approximate quantiles for massive data\"\n",
    "]\n",
    "\n",
    "for tip in optimization_tips:\n",
    "    print(f\"   {tip}\")\n",
    "\n",
    "# Integration best practices\n",
    "print(f\"\\nüîó Integration Best Practices:\")\n",
    "\n",
    "integration_practices = {\n",
    "    \"Pipeline Design\": [\n",
    "        \"Place binning before scaling/normalization\",\n",
    "        \"Use ColumnTransformer for mixed data types\",\n",
    "        \"Include binning in cross-validation\",\n",
    "        \"Test pipeline end-to-end\"\n",
    "    ],\n",
    "    \n",
    "    \"Model Training\": [\n",
    "        \"Compare binned vs unbinned features\",\n",
    "        \"Use binning to handle outliers\",\n",
    "        \"Consider binning for regularization\",\n",
    "        \"Validate on representative data\"\n",
    "    ],\n",
    "    \n",
    "    \"Production Deployment\": [\n",
    "        \"Version control binning models\",\n",
    "        \"Monitor input data distribution\",\n",
    "        \"Handle new data edge cases\",\n",
    "        \"Document bin interpretation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in integration_practices.items():\n",
    "    print(f\"\\n   {category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"      ‚Ä¢ {practice}\")\n",
    "\n",
    "# Create decision tree visualization\n",
    "print(f\"\\nüå≥ Decision Tree for Binning Method Selection:\")\n",
    "\n",
    "decision_tree = \"\"\"\n",
    "Data Distribution Analysis\n",
    "‚îú‚îÄ‚îÄ Highly Skewed (|skewness| > 2)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ With Outliers ‚Üí EqualFrequencyBinning ‚≠ê\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Without Outliers ‚Üí EqualFrequencyBinning ‚≠ê\n",
    "‚îú‚îÄ‚îÄ Moderately Skewed (1 < |skewness| ‚â§ 2)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Domain Knowledge Available ‚Üí Manual Binning\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ No Domain Knowledge ‚Üí EqualFrequencyBinning\n",
    "‚îú‚îÄ‚îÄ Approximately Normal (|skewness| ‚â§ 1)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Interpretability Critical ‚Üí EqualWidthBinning\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Statistical Balance Critical ‚Üí EqualFrequencyBinning\n",
    "‚îî‚îÄ‚îÄ Unknown Distribution\n",
    "    ‚îî‚îÄ‚îÄ Start with EqualFrequencyBinning (robust default)\n",
    "\"\"\"\n",
    "\n",
    "print(decision_tree)\n",
    "\n",
    "# Final recommendation summary\n",
    "print(f\"\\nüéØ Executive Summary:\")\n",
    "summary_points = [\n",
    "    \"EqualFrequencyBinning is the robust choice for skewed data\",\n",
    "    \"Provides statistical balance across bins\",\n",
    "    \"Handles outliers naturally and effectively\", \n",
    "    \"Integrates seamlessly with scikit-learn pipelines\",\n",
    "    \"Start with 5 bins and tune via cross-validation\",\n",
    "    \"Monitor data distribution changes in production\",\n",
    "    \"Combine with domain knowledge when available\"\n",
    "]\n",
    "\n",
    "for i, point in enumerate(summary_points, 1):\n",
    "    print(f\"   {i}. {point}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to implement EqualFrequencyBinning in your workflow!\")\n",
    "print(f\"   Remember: The best binning method depends on your specific data and use case.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1fca23",
   "metadata": {},
   "source": [
    "## üéØ Conclusion\n",
    "\n",
    "This comprehensive exploration of EqualFrequencyBinning demonstrates its effectiveness as a robust data preprocessing technique, particularly for handling skewed distributions and outlier-heavy datasets. The method's ability to maintain statistical balance while adapting to data characteristics makes it a valuable tool in the data scientist's toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final conclusion and key takeaways\n",
    "print(\"üéØ EqualFrequencyBinning: Comprehensive Analysis Conclusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üìä What We've Learned:\")\n",
    "\n",
    "key_findings = {\n",
    "    \"Core Strengths\": [\n",
    "        \"‚úÖ Excellent handling of skewed distributions\",\n",
    "        \"‚úÖ Natural robustness to outliers and extreme values\",\n",
    "        \"‚úÖ Maintains statistical balance across bins\", \n",
    "        \"‚úÖ Seamless scikit-learn pipeline integration\",\n",
    "        \"‚úÖ Competitive computational performance\",\n",
    "        \"‚úÖ Handles most edge cases gracefully\"\n",
    "    ],\n",
    "    \n",
    "    \"Key Applications\": [\n",
    "        \"üéØ Financial data with heavy-tailed distributions\",\n",
    "        \"üéØ Sensor data with outliers and measurement errors\",\n",
    "        \"üéØ Web analytics with power-law user behavior\",\n",
    "        \"üéØ Preprocessing for algorithms sensitive to outliers\",\n",
    "        \"üéØ Feature engineering for tree-based models\",\n",
    "        \"üéØ Creating balanced categorical representations\"\n",
    "    ],\n",
    "    \n",
    "    \"Technical Insights\": [\n",
    "        \"üî¨ Per-column vs joint fitting modes offer flexibility\",\n",
    "        \"üî¨ Bin count selection impacts granularity vs stability\",\n",
    "        \"üî¨ Memory overhead is reasonable for most applications\",\n",
    "        \"üî¨ Performance scales linearly with data size and features\",\n",
    "        \"üî¨ Edge cases require minimal special handling\",\n",
    "        \"üî¨ Cross-validation helps optimize bin count selection\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, points in key_findings.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for point in points:\n",
    "        print(f\"   {point}\")\n",
    "\n",
    "# When to choose EqualFrequencyBinning\n",
    "print(f\"\\nüéØ Choose EqualFrequencyBinning When:\")\n",
    "choose_when = [\n",
    "    \"Your data exhibits significant skewness (|skewness| > 1)\",\n",
    "    \"Outliers are present and difficult to remove\",\n",
    "    \"You need balanced sample sizes across bins\",\n",
    "    \"Statistical stability is more important than interpretability\",\n",
    "    \"Working with heavy-tailed or power-law distributions\",\n",
    "    \"Building models sensitive to feature distributions\"\n",
    "]\n",
    "\n",
    "for i, condition in enumerate(choose_when, 1):\n",
    "    print(f\"   {i}. {condition}\")\n",
    "\n",
    "# When to consider alternatives\n",
    "print(f\"\\nü§î Consider Alternatives When:\")\n",
    "consider_alternatives = [\n",
    "    \"Data is approximately normally distributed\",\n",
    "    \"Domain-specific bin boundaries are required\",\n",
    "    \"Interpretability of bin ranges is critical\",\n",
    "    \"Real-time applications need consistent boundaries\",\n",
    "    \"Many duplicate values prevent equal frequencies\"\n",
    "]\n",
    "\n",
    "for i, condition in enumerate(consider_alternatives, 1):\n",
    "    print(f\"   {i}. {condition}\")\n",
    "\n",
    "# Implementation checklist\n",
    "print(f\"\\n‚úÖ Implementation Checklist:\")\n",
    "checklist = [\n",
    "    \"Analyze data distribution characteristics\",\n",
    "    \"Handle missing values before binning\",\n",
    "    \"Choose appropriate bin count (typically 3-10)\",\n",
    "    \"Validate with cross-validation\",\n",
    "    \"Test edge cases and robustness\",\n",
    "    \"Monitor performance in production\",\n",
    "    \"Document binning strategy and rationale\"\n",
    "]\n",
    "\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"   {i}. {item}\")\n",
    "\n",
    "# Future considerations\n",
    "print(f\"\\nüîÆ Future Considerations:\")\n",
    "future_items = [\n",
    "    \"Monitor data distribution drift over time\",\n",
    "    \"Consider adaptive binning for streaming data\",\n",
    "    \"Experiment with hybrid binning approaches\",\n",
    "    \"Evaluate impact on model interpretability\",\n",
    "    \"Explore integration with automated ML pipelines\"\n",
    "]\n",
    "\n",
    "for item in future_items:\n",
    "    print(f\"   ‚Ä¢ {item}\")\n",
    "\n",
    "# Final summary statistics from our analysis\n",
    "print(f\"\\nüìà Analysis Summary Statistics:\")\n",
    "summary_stats = {\n",
    "    \"Datasets Analyzed\": \"4 (skewed, outlier-heavy, mixed, classification)\",\n",
    "    \"Bin Configurations Tested\": \"Multiple (3, 5, 10, 20 bins)\",\n",
    "    \"Performance Benchmarks\": \"Data size, feature count, bin count scaling\",\n",
    "    \"Edge Cases Evaluated\": \"6 (duplicates, small data, outliers, constants, NaN, limited unique)\",\n",
    "    \"Pipeline Integrations\": \"5 (no binning, EqualFreq 3/5/10, EqualWidth 5)\",\n",
    "    \"Cross-validation Folds\": \"5-fold for robust evaluation\"\n",
    "}\n",
    "\n",
    "for metric, value in summary_stats.items():\n",
    "    print(f\"   {metric}: {value}\")\n",
    "\n",
    "print(f\"\\nüåü Final Recommendation:\")\n",
    "print(\"   EqualFrequencyBinning is a robust, versatile preprocessing tool\")\n",
    "print(\"   that excels with skewed data and provides statistical balance.\")\n",
    "print(\"   It should be a primary consideration for data preprocessing\")\n",
    "print(\"   pipelines, especially when dealing with real-world messy data.\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"Apply to your specific dataset\",\n",
    "    \"Compare with other binning methods\",\n",
    "    \"Integrate into your ML pipeline\",\n",
    "    \"Monitor and validate in production\",\n",
    "    \"Share insights with your team\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"   {i}. {step}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üìù Thank you for exploring EqualFrequencyBinning!\")\n",
    "print(\"   This analysis provides a comprehensive foundation for\")\n",
    "print(\"   effective use in your data science projects.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b8f9f",
   "metadata": {},
   "source": [
    "## 3. EqualFrequency vs EqualWidth Comparison\n",
    "\n",
    "Let's directly compare EqualFrequencyBinning with EqualWidthBinning to understand the key differences and advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è EqualFrequency vs EqualWidth Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import EqualWidthBinning for comparison\n",
    "from binlearn.methods import EqualWidthBinning\n",
    "\n",
    "# Use the highly skewed data for comparison\n",
    "test_features = ['exponential', 'lognormal']\n",
    "comparison_data = data_skewed[test_features].copy()\n",
    "\n",
    "print(f\"üìä Testing on highly skewed features: {test_features}\")\n",
    "print(f\"Data shape: {comparison_data.shape}\")\n",
    "\n",
    "# Compare both methods\n",
    "n_bins = 5\n",
    "comparison_results = {}\n",
    "\n",
    "methods = {\n",
    "    'EqualFrequency': EqualFrequencyBinning(n_bins=n_bins),\n",
    "    'EqualWidth': EqualWidthBinning(n_bins=n_bins)\n",
    "}\n",
    "\n",
    "for method_name, binner in methods.items():\n",
    "    print(f\"\\nüîç {method_name} Analysis:\")\n",
    "    \n",
    "    # Fit and transform\n",
    "    binner.fit(comparison_data)\n",
    "    transformed = binner.transform(comparison_data)\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results[method_name] = {\n",
    "        'binner': binner,\n",
    "        'transformed': transformed,\n",
    "        'bin_edges': binner.bin_edges_\n",
    "    }\n",
    "    \n",
    "    # Analyze bin populations for each feature\n",
    "    for i, feature in enumerate(test_features):\n",
    "        binned_values = transformed[:, i]\n",
    "        unique_bins, counts = np.unique(binned_values, return_counts=True)\n",
    "        \n",
    "        print(f\"  üìà {feature}:\")\n",
    "        print(f\"     Bin edges: {np.round(binner.bin_edges_[feature], 3)}\")\n",
    "        print(f\"     Bin sizes: {counts} (total: {sum(counts)})\")\n",
    "        print(f\"     Size std:  {np.std(counts):.2f} (lower = more balanced)\")\n",
    "\n",
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for row, feature in enumerate(test_features):\n",
    "    feature_data = comparison_data[feature].values\n",
    "    \n",
    "    # Original distribution\n",
    "    ax = axes[row, 0]\n",
    "    ax.hist(feature_data, bins=30, alpha=0.7, color='lightgray', edgecolor='black')\n",
    "    ax.set_title(f'Original {feature}\\n(Skewness: {stats.skew(feature_data):.2f})')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compare binning methods\n",
    "    for col, (method_name, result) in enumerate(comparison_results.items(), 1):\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Get binned values for this feature\n",
    "        feature_idx = list(comparison_data.columns).index(feature)\n",
    "        binned_values = result['transformed'][:, feature_idx]\n",
    "        bin_edges = result['bin_edges'][feature]\n",
    "        \n",
    "        # Create histogram with bin boundaries\n",
    "        ax.hist(feature_data, bins=30, alpha=0.6, color='lightblue', \n",
    "               edgecolor='black', label='Original')\n",
    "        \n",
    "        # Show bin boundaries\n",
    "        colors = ['red', 'blue']\n",
    "        for edge in bin_edges:\n",
    "            ax.axvline(edge, linestyle='--', alpha=0.8, \n",
    "                      color=colors[col-1], linewidth=2, \n",
    "                      label='Bin edge' if edge == bin_edges[0] else '')\n",
    "        \n",
    "        # Calculate and show bin populations\n",
    "        bin_counts = []\n",
    "        for i in range(len(bin_edges)-1):\n",
    "            mask = (feature_data >= bin_edges[i]) & (feature_data < bin_edges[i+1])\n",
    "            bin_counts.append(np.sum(mask))\n",
    "        # Handle last bin (include upper boundary)\n",
    "        mask = feature_data >= bin_edges[-2]\n",
    "        bin_counts[-1] = np.sum(mask)\n",
    "        \n",
    "        ax.set_title(f'{method_name} - {feature}\\\\nBin sizes: {bin_counts}\\\\nStd: {np.std(bin_counts):.1f}')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantitative comparison\n",
    "print(f\"\\nüìä Quantitative Comparison Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<15} {'Feature':<12} {'Bin Balance':<12} {'Edge Range':<15} {'Performance':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for method_name, result in comparison_results.items():\n",
    "    for feature in test_features:\n",
    "        # Calculate bin balance (lower std = more balanced)\n",
    "        feature_idx = list(comparison_data.columns).index(feature)\n",
    "        binned_values = result['transformed'][:, feature_idx]\n",
    "        unique_bins, counts = np.unique(binned_values, return_counts=True)\n",
    "        balance_score = np.std(counts)\n",
    "        \n",
    "        # Calculate edge range\n",
    "        edges = result['bin_edges'][feature]\n",
    "        edge_range = edges[-1] - edges[0]\n",
    "        \n",
    "        # Performance indicator\n",
    "        performance = \"üèÜ Better\" if method_name == \"EqualFrequency\" and balance_score < 10 else \"‚ö†Ô∏è Worse\" if balance_score > 50 else \"‚úÖ Good\"\n",
    "        \n",
    "        print(f\"{method_name:<15} {feature:<12} {balance_score:<12.1f} {edge_range:<15.2f} {performance:<12}\")\n",
    "\n",
    "print(f\"\\nüîç Key Insights:\")\n",
    "print(f\"   üìä EqualFrequency creates more balanced bin populations\")\n",
    "print(f\"   üìè EqualWidth creates uniform bin widths but uneven populations\")\n",
    "print(f\"   üéØ For skewed data, EqualFrequency prevents empty or overcrowded bins\")\n",
    "print(f\"   ‚öñÔ∏è Bin balance measured by standard deviation of bin sizes (lower = better)\")\n",
    "print(f\"   üèÜ EqualFrequency typically achieves balance_score < 10 for most data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75e6c3",
   "metadata": {},
   "source": [
    "## 3. Basic Equal Frequency Binning\n",
    "\n",
    "Let's start with the fundamental usage of `EqualFrequencyBinning` using the fit/transform pattern. Equal frequency binning ensures each bin contains approximately the same number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c878d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating EqualFrequencyBinning instance...\n",
      "\n",
      "üéØ Fitting binner on DataFrame with skewed data...\n",
      "\n",
      "üìã Fitted parameters (quantile-based bin edges):\n",
      "Number of features: 4\n",
      "  age: 5 bins\n",
      "    Edges: [18.44 22.61 25.78 28.15 33.9  55.01]\n",
      "    Quantiles: [0.  0.2 0.4 0.6 0.8 1. ]\n",
      "  income: 5 bins\n",
      "    Edges: [  2395.03  11273.31  17268.1   27493.25  44045.55 138735.99]\n",
      "    Quantiles: [0.  0.2 0.4 0.6 0.8 1. ]\n",
      "  score: 5 bins\n",
      "    Edges: [ 0.72 13.84 23.06 30.29 42.03 73.81]\n",
      "    Quantiles: [0.  0.2 0.4 0.6 0.8 1. ]\n",
      "  wait_time: 5 bins\n",
      "    Edges: [ 0.    0.78  1.54  2.97  4.6  12.57]\n",
      "    Quantiles: [0.  0.2 0.4 0.6 0.8 1. ]\n",
      "\n",
      "üîÑ Transforming data...\n",
      "Original DataFrame shape: (200, 4)\n",
      "Binned DataFrame shape: (200, 4)\n",
      "Original data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Binned data type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "üîç Sample transformations:\n",
      "Row 0:\n",
      "  age: 31.21 ‚Üí bin 3\n",
      "  income: 4535.21 ‚Üí bin 0\n",
      "  score: 24.40 ‚Üí bin 2\n",
      "  wait_time: 6.69 ‚Üí bin 4\n",
      "\n",
      "Row 1:\n",
      "  age: 38.19 ‚Üí bin 4\n",
      "  income: 27629.51 ‚Üí bin 3\n",
      "  score: 39.36 ‚Üí bin 3\n",
      "  wait_time: 1.63 ‚Üí bin 2\n",
      "\n",
      "Row 2:\n",
      "  age: 27.67 ‚Üí bin 2\n",
      "  income: 11676.98 ‚Üí bin 1\n",
      "  score: 48.72 ‚Üí bin 4\n",
      "  wait_time: 4.55 ‚Üí bin 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a basic EqualFrequencyBinning instance\n",
    "print(\"üîß Creating EqualFrequencyBinning instance...\")\n",
    "basic_binner = EqualFrequencyBinning(n_bins=5, preserve_dataframe=True)\n",
    "\n",
    "# Fit the binner on DataFrame data (skewed distributions)\n",
    "print(\"\\nüéØ Fitting binner on DataFrame with skewed data...\")\n",
    "basic_binner.fit(df_data)\n",
    "\n",
    "# Check the fitted parameters - quantile-based edges\n",
    "print(\"\\nüìã Fitted parameters (quantile-based bin edges):\")\n",
    "print(f\"Number of features: {len(basic_binner.bin_edges_)}\")\n",
    "for feature, edges in basic_binner.bin_edges_.items():\n",
    "    print(f\"  {feature}: {len(edges)-1} bins\")\n",
    "    print(f\"    Edges: {np.round(edges, 2)}\")\n",
    "    \n",
    "    # Show quantiles used\n",
    "    quantiles = np.linspace(0, 1, len(edges))\n",
    "    print(f\"    Quantiles: {np.round(quantiles, 3)}\")\n",
    "\n",
    "# Transform the data\n",
    "print(\"\\nüîÑ Transforming data...\")\n",
    "df_binned = basic_binner.transform(df_data)\n",
    "\n",
    "print(f\"Original DataFrame shape: {df_data.shape}\")\n",
    "print(f\"Binned DataFrame shape: {df_binned.shape}\")\n",
    "print(f\"Original data type: {type(df_data)}\")\n",
    "print(f\"Binned data type: {type(df_binned)}\")\n",
    "\n",
    "# Show sample transformations\n",
    "print(\"\\nüîç Sample transformations:\")\n",
    "for i in range(3):\n",
    "    print(f\"Row {i}:\")\n",
    "    for col in df_data.columns:\n",
    "        original = df_data.iloc[i][col]\n",
    "        binned = df_binned.iloc[i][col]\n",
    "        print(f\"  {col}: {original:.2f} ‚Üí bin {binned}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
